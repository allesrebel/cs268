{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f858500d",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "Alles Rebel\n",
    "Computational Science PhD\n",
    "\n",
    "\n",
    "\n",
    "## Intoduction!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef4935",
   "metadata": {},
   "source": [
    "## Pre-Reqs\n",
    "\n",
    "Like other homeworks, we'll start by getting the kernel set up with the right dependencies. Running this once should do the trick for the rest of the homework;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd4f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "   3163.9 ms\u001b[32m  ✓ \u001b[39mPlots → IJuliaExt\n",
      "  1 dependency successfully precompiled in 5 seconds. 240 already precompiled.\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"PrettyTables\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"ForwardDiff\")\n",
    "Pkg.add(\"Optim\")\n",
    "\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using Random\n",
    "using Distributions\n",
    "using Statistics\n",
    "using Dates\n",
    "using PrettyTables\n",
    "using Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90552b6e",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "The goal of this assignment is to apply a gradient solver to a constrainted optimization problem. In this case the problem needs to be atleast 10 dimensions. The method I'll stick with is the first one suggested by the assignment: the method of Augmented Lagrangian! And we'll apply it to Booth's Function, just updated to use 10 variables instead of 2 via the chaining method introduced in HW2, 1C.\n",
    "\n",
    "### Broad Overview of Augmented Lagrangians\n",
    "In general, these methods solve optimization problems with constraints by incorporating the constraints as additional terms in the objective function. These additional terms involve new parameters, known as Lagrange multipliers, which help reformulate the constrained problem (involving equalities or inequalities) into an unconstrained one.\n",
    "\n",
    "To ensure numerical stablity - Augmented lagrangians intoduce a penality term to the discourage constraint violations. The optimization alternates between minimizing the augmented Lagrangian and updating the multipliers and penalty parameters iteratively. That's what we'll be implementing below!\n",
    "\n",
    "### Booth's Function!\n",
    "\n",
    "We'll reuse the code I wrote from the earlier homework - line for line. Originally designed to be used with existing autodiff (forwardiff in julia) and Optim package. It will suit our purposes for this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7a4f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Booth's function - generalized for any number of variables\n",
    "# Uses the method suggested by Homework2 Problem 1C to chain variables\n",
    "# pair wise to extend from 2D -> ND\n",
    "# I wrote this for HW2\n",
    "function booths_n(x; n = 10)\n",
    "    result = 0.0\n",
    "    # same rationale for ending at n-1, we need the nth element for n-1\n",
    "    for i in 1:(n - 1)\n",
    "        result += ((x[i] + 2*x[i+1] - 7)^2 + (2*x[i] + x[i+1] - 5)^2)\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "# Same with this - since the methods of HW needed gradients\n",
    "# we'll need this as well for this HW\n",
    "function booths_n_gradient(x; n = length(x))\n",
    "    grad = zeros(length(x)) # modified to be typeless\n",
    "    \n",
    "    # Special case for the first element\n",
    "    grad[1] = 2*(x[1] + 2*x[2] - 7) + 4*(2*x[1] + x[2] - 5)\n",
    "    \n",
    "    # Compute the gradient for the 2:n-1 elements\n",
    "    for i in 2:(n-1)\n",
    "        grad[i] = 2*(x[i]+2*x[i+1]-7) + 4*(2*x[i] + x[i+1] - 5) + 4*(x[i-1] + 2*x[i] - 7) + 2*(2*x[i-1] + x[i] - 5)\n",
    "    end\n",
    "\n",
    "    # Special case for the last element (n-th element)\n",
    "    grad[n] = 4*(x[n-1] + 2*x[n] - 7) + 2*(2*x[n-1] + x[n] - 5) \n",
    "    \n",
    "    return grad\n",
    "end\n",
    "\n",
    "# since this is a constraints optimization problem, we'll add something here\n",
    "\n",
    "# Define the constraint h(x)\n",
    "function h(x)\n",
    "    return [sum(x)]  # For example, we can constrain the sum of x to zero\n",
    "end    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f5849",
   "metadata": {},
   "source": [
    "### Implemention of augmented lagrangian method\n",
    "\n",
    "This will be a bit involved, but in general - it's the following steps:\n",
    "- We'll attempt to use Julia's Optim library for the minimization method\n",
    "- We'll instrument up the result to match the results from earlier\n",
    "(function calls, grad calls, convergement measure, optimization point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7f168c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augmented_lagrange_method (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Augmented Lagrangian Method using Optim.optimize\n",
    "# Based upon the code in Algorithms for Optimization (Algorithm 10.2 of K&W)\n",
    "# Modified to keep track of function and grad calls\n",
    "function augmented_lagrange_method(f, h, x0, k_max; ρ=1.0, γ=2.0, tol=1e-6)\n",
    "    x = x0\n",
    "    λ = zeros(length(h(x)))               # Initialize Lagrange multipliers\n",
    "    old_f_value = f(x)                    # Track old objective value\n",
    "    convergence_measure = 0.0             # Convergence measure\n",
    "\n",
    "    total_f_calls = 0   # Total function evaluations\n",
    "    total_g_calls = 0   # Total gradient evaluations\n",
    "\n",
    "    for k in 1:k_max\n",
    "        # Define penalty function\n",
    "        function p(x)\n",
    "            λ_promoted = convert.(eltype(x[1]), λ)\n",
    "            return (ρ / 2) * sum(h(x).^2) - dot(λ_promoted, h(x))\n",
    "        end\n",
    "\n",
    "        # Define augmented objective function\n",
    "        function augmented_objective(x)\n",
    "            return f(x) + p(x)\n",
    "        end\n",
    "\n",
    "        # Optimize using Optim.optimize\n",
    "        result = optimize(augmented_objective, x, method=LBFGS(); autodiff=:forward)\n",
    "        x = Optim.minimizer(result)\n",
    "\n",
    "        # Accumulate function and gradient evaluations\n",
    "        total_f_calls += result.f_calls\n",
    "        total_g_calls += result.g_calls\n",
    "\n",
    "        # Update convergence metrics\n",
    "        new_f_value = f(x)\n",
    "        convergence_measure = abs(new_f_value - old_f_value)\n",
    "        old_f_value = new_f_value\n",
    "\n",
    "        # Check for convergence\n",
    "        if convergence_measure < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # Update Lagrange multipliers and penalty parameter\n",
    "        λ .= λ .- ρ * ForwardDiff.value.(h(x))\n",
    "        ρ *= γ\n",
    "    end\n",
    "\n",
    "    return x, total_f_calls, total_g_calls\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135eb6ca",
   "metadata": {},
   "source": [
    "### Experimental Methodology\n",
    "\n",
    "We'll utilize the same testing methodology as the HW2, generating a BUNCH of random vectors to start from. And then running the method across each of the vectors, recording how many function calls, gradient calls, exectuation time and error from what the ideal solution would have been. We'll compare the above method against the earlier line search method.\n",
    "\n",
    "First we'll get the environment ready to do the experiment - how we'll generate everything an calculate the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ca529c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code we used earlier in HW2! This will be what we'll compare against\n",
    "# I wrote this code with the book as the primary sources\n",
    "# Conjugate Gradient Descent method (with function and gradient counts)\n",
    "# Updated tracking - and fixed accounting issues missed in originial HW2 implementation\n",
    "mutable struct ConjugateGradientDescent\n",
    "    d::Vector{Float64}\n",
    "    g::Vector{Float64}\n",
    "    iterations::Int\n",
    "    func_calls::Int\n",
    "    grad_calls::Int\n",
    "    converged::Bool\n",
    "end\n",
    "\n",
    "function init!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    M.g = ∇f(x)\n",
    "    M.d = -M.g\n",
    "    M.iterations = 0\n",
    "    M.func_calls = 1\n",
    "    M.grad_calls = 1\n",
    "    M.converged = false\n",
    "    return M\n",
    "end\n",
    "\n",
    "function backtracking_line_search(f, ∇f, x, d, α; p=0.5, β=1e-4)\n",
    "    y, g = f(x), ∇f(x)\n",
    "    func_calls = 1\n",
    "    grad_calls = 1\n",
    "    while f(x + α*d) > y + β*α*(g⋅d)\n",
    "        α *= p\n",
    "        func_calls += 1\n",
    "    end\n",
    "    return α, func_calls, grad_calls\n",
    "end\n",
    "\n",
    "function step!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    d, g = M.d, M.g\n",
    "    α, f_calls_ls, g_calls_ls = backtracking_line_search(f, ∇f, x, d, 1.0)  # initial α can be 1.0\n",
    "    x_new = x + α * d\n",
    "    M.func_calls += f_calls_ls\n",
    "    M.grad_calls += g_calls_ls\n",
    "    M.func_calls += 1  # For evaluating f at x_new\n",
    "    g_new = ∇f(x_new)\n",
    "    M.grad_calls += 1\n",
    "    β = max(0, dot(g_new, g_new - g) / dot(g, g))\n",
    "    d_new = -g_new + β * d\n",
    "    M.d = d_new\n",
    "    M.g = g_new\n",
    "    M.iterations += 1\n",
    "    if norm(g_new) < 1e-6\n",
    "        M.converged = true\n",
    "    end\n",
    "    return x_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03046e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_initial_points (generic function with 5 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_initial_points(left_limit=-10.0, right_limit=10.0, samples=100, dims=10)\n",
    "    # Generate 100 random real-valued starting points as specified\n",
    "    initial_points = [] \n",
    "\n",
    "    for i in 1:samples # defaults to 100 samples\n",
    "        this_sample = []\n",
    "        \n",
    "        for j in 1:dims\n",
    "            # y is uniformly picked in [-10.0, 10.0] by default\n",
    "            # note, rand is already uniform, just need to cap the limits\n",
    "            y = rand(Uniform(left_limit, right_limit))\n",
    "            # choose + or - with equal probability, we'll use bitrand (50/50)\n",
    "            sign = bitrand() == 0 ? -1 : 1\n",
    "            x = sign * exp(y)\n",
    "            push!(this_sample, x)\n",
    "        end\n",
    "        \n",
    "        push!(initial_points, this_sample)\n",
    "    end\n",
    "\n",
    "    return initial_points\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8756bd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Conjugate Gradient Descent on Booth's function:\n",
      "Run 1: Absolute Error = 2.2235711272244885e-8, Converged = true, Function Calls = 2307, Gradient Calls = 225, Time = 0.17394399642944336\n",
      "Run 2: Absolute Error = 4.117869822906073e-8, Converged = true, Function Calls = 1337, Gradient Calls = 165, Time = 0.0002181529998779297\n",
      "Run 3: Absolute Error = 3.744888488199649e-8, Converged = true, Function Calls = 1374, Gradient Calls = 161, Time = 0.00020194053649902344\n",
      "Run 4: Absolute Error = 1.64623950027476e-8, Converged = true, Function Calls = 1442, Gradient Calls = 179, Time = 0.0002110004425048828\n",
      "Run 5: Absolute Error = 2.902422346906519e-8, Converged = true, Function Calls = 1303, Gradient Calls = 183, Time = 0.0002219676971435547\n",
      "Run 6: Absolute Error = 8.629066350351877e-9, Converged = true, Function Calls = 1271, Gradient Calls = 157, Time = 0.00018310546875\n",
      "Run 7: Absolute Error = 3.844521079265917e-8, Converged = true, Function Calls = 1562, Gradient Calls = 161, Time = 0.0002129077911376953\n",
      "Run 8: Absolute Error = 6.070312164041525e-8, Converged = true, Function Calls = 1812, Gradient Calls = 175, Time = 0.00024008750915527344\n",
      "Run 9: Absolute Error = 3.074288912330303e-8, Converged = true, Function Calls = 1916, Gradient Calls = 213, Time = 0.0002808570861816406\n",
      "Run 10: Absolute Error = 2.5984343121621123e-8, Converged = true, Function Calls = 1213, Gradient Calls = 165, Time = 0.000186920166015625\n",
      "Run 11: Absolute Error = 2.5554540261651937e-8, Converged = true, Function Calls = 1118, Gradient Calls = 139, Time = 0.00016379356384277344\n",
      "Run 12: Absolute Error = 6.478050917912981e-8, Converged = true, Function Calls = 1098, Gradient Calls = 129, Time = 0.0001590251922607422\n",
      "Run 13: Absolute Error = 3.285230842919873e-8, Converged = true, Function Calls = 1544, Gradient Calls = 179, Time = 0.00022292137145996094\n",
      "Run 14: Absolute Error = 5.350836174145002e-8, Converged = true, Function Calls = 1298, Gradient Calls = 159, Time = 0.0001888275146484375\n",
      "Run 15: Absolute Error = 3.712584506487815e-8, Converged = true, Function Calls = 1645, Gradient Calls = 187, Time = 0.00022912025451660156\n",
      "Run 16: Absolute Error = 2.648628871781966e-9, Converged = true, Function Calls = 1520, Gradient Calls = 179, Time = 0.00021910667419433594\n",
      "Run 17: Absolute Error = 3.107823598469395e-8, Converged = true, Function Calls = 1613, Gradient Calls = 173, Time = 0.0002219676971435547\n",
      "Run 18: Absolute Error = 4.783175988620769e-8, Converged = true, Function Calls = 1205, Gradient Calls = 151, Time = 0.0001800060272216797\n",
      "Run 19: Absolute Error = 2.481750582816744e-8, Converged = true, Function Calls = 978, Gradient Calls = 133, Time = 0.00015807151794433594\n",
      "Run 20: Absolute Error = 4.551344812142588e-8, Converged = true, Function Calls = 1021, Gradient Calls = 143, Time = 0.0001590251922607422\n",
      "Run 21: Absolute Error = 2.0410762413547445e-8, Converged = true, Function Calls = 2111, Gradient Calls = 213, Time = 0.00028896331787109375\n",
      "Run 22: Absolute Error = 1.604835908608493e-8, Converged = true, Function Calls = 1621, Gradient Calls = 179, Time = 0.0002300739288330078\n",
      "Run 23: Absolute Error = 2.2933471122144056e-8, Converged = true, Function Calls = 1179, Gradient Calls = 141, Time = 0.00016808509826660156\n",
      "Run 24: Absolute Error = 1.7900922744118475e-8, Converged = true, Function Calls = 1492, Gradient Calls = 161, Time = 0.00021314620971679688\n",
      "Run 25: Absolute Error = 4.969638611740379e-8, Converged = true, Function Calls = 1568, Gradient Calls = 165, Time = 0.0002079010009765625\n",
      "Run 26: Absolute Error = 2.3335660070955555e-8, Converged = true, Function Calls = 1289, Gradient Calls = 175, Time = 0.00019288063049316406\n",
      "Run 27: Absolute Error = 1.0152325202739121e-8, Converged = true, Function Calls = 1769, Gradient Calls = 195, Time = 0.00921010971069336\n",
      "Run 28: Absolute Error = 4.3362690993475894e-8, Converged = true, Function Calls = 1643, Gradient Calls = 193, Time = 0.00022602081298828125\n",
      "Run 29: Absolute Error = 2.071226257172043e-8, Converged = true, Function Calls = 1254, Gradient Calls = 143, Time = 0.0001800060272216797\n",
      "Run 30: Absolute Error = 4.842179368580446e-8, Converged = true, Function Calls = 1171, Gradient Calls = 159, Time = 0.00017213821411132812\n",
      "Run 31: Absolute Error = 8.406839668850807e-8, Converged = true, Function Calls = 1770, Gradient Calls = 181, Time = 0.00023102760314941406\n",
      "Run 32: Absolute Error = 2.037239177354877e-8, Converged = true, Function Calls = 1260, Gradient Calls = 177, Time = 0.00018596649169921875\n",
      "Run 33: Absolute Error = 5.452698914609755e-8, Converged = true, Function Calls = 1201, Gradient Calls = 159, Time = 0.00017404556274414062\n",
      "Run 34: Absolute Error = 7.360566889325071e-8, Converged = true, Function Calls = 1675, Gradient Calls = 175, Time = 0.00022912025451660156\n",
      "Run 35: Absolute Error = 7.400033918969484e-9, Converged = true, Function Calls = 1442, Gradient Calls = 187, Time = 0.00020503997802734375\n",
      "Run 36: Absolute Error = 6.887422410173372e-8, Converged = true, Function Calls = 1467, Gradient Calls = 169, Time = 0.0002071857452392578\n",
      "Run 37: Absolute Error = 4.5613283372603064e-8, Converged = true, Function Calls = 1252, Gradient Calls = 147, Time = 0.00017309188842773438\n",
      "Run 38: Absolute Error = 6.474297165048881e-8, Converged = true, Function Calls = 1373, Gradient Calls = 159, Time = 0.0001850128173828125\n",
      "Run 39: Absolute Error = 1.4624277255848028e-7, Converged = true, Function Calls = 1416, Gradient Calls = 173, Time = 0.00020599365234375\n",
      "Run 40: Absolute Error = 6.396439666822573e-8, Converged = true, Function Calls = 1384, Gradient Calls = 157, Time = 0.00018596649169921875\n",
      "Run 41: Absolute Error = 1.6155471627143925e-8, Converged = true, Function Calls = 1262, Gradient Calls = 165, Time = 0.00018095970153808594\n",
      "Run 42: Absolute Error = 6.818472897407446e-9, Converged = true, Function Calls = 1512, Gradient Calls = 189, Time = 0.0002110004425048828\n",
      "Run 43: Absolute Error = 1.4676782456035653e-8, Converged = true, Function Calls = 1345, Gradient Calls = 169, Time = 0.00019097328186035156\n",
      "Run 44: Absolute Error = 8.109843796511029e-8, Converged = true, Function Calls = 1137, Gradient Calls = 145, Time = 0.00017023086547851562\n",
      "Run 45: Absolute Error = 8.821941777092945e-8, Converged = true, Function Calls = 1582, Gradient Calls = 179, Time = 0.0002129077911376953\n",
      "Run 46: Absolute Error = 2.9662571954247596e-8, Converged = true, Function Calls = 1720, Gradient Calls = 195, Time = 0.00023102760314941406\n",
      "Run 47: Absolute Error = 1.0770575098462132e-8, Converged = true, Function Calls = 1351, Gradient Calls = 161, Time = 0.00018596649169921875\n",
      "Run 48: Absolute Error = 1.3223322437028173e-8, Converged = true, Function Calls = 1327, Gradient Calls = 151, Time = 0.0001800060272216797\n",
      "Run 49: Absolute Error = 4.079333271178598e-8, Converged = true, Function Calls = 1295, Gradient Calls = 161, Time = 0.00019812583923339844\n",
      "Run 50: Absolute Error = 1.1143510780797783e-8, Converged = true, Function Calls = 1903, Gradient Calls = 213, Time = 0.00025391578674316406\n",
      "Run 51: Absolute Error = 4.884885007605533e-8, Converged = true, Function Calls = 1574, Gradient Calls = 163, Time = 0.00020885467529296875\n",
      "Run 52: Absolute Error = 1.4679175652787535e-8, Converged = true, Function Calls = 1646, Gradient Calls = 189, Time = 0.0002238750457763672\n",
      "Run 53: Absolute Error = 3.696567563160613e-8, Converged = true, Function Calls = 1191, Gradient Calls = 157, Time = 0.00017189979553222656\n",
      "Run 54: Absolute Error = 3.510420731878128e-8, Converged = true, Function Calls = 1942, Gradient Calls = 207, Time = 0.00026106834411621094\n",
      "Run 55: Absolute Error = 2.030528190033465e-8, Converged = true, Function Calls = 2001, Gradient Calls = 207, Time = 0.00026297569274902344\n",
      "Run 56: Absolute Error = 6.702138843905914e-9, Converged = true, Function Calls = 1655, Gradient Calls = 183, Time = 0.0002219676971435547\n",
      "Run 57: Absolute Error = 7.928462331108221e-9, Converged = true, Function Calls = 1312, Gradient Calls = 189, Time = 0.00019812583923339844\n",
      "Run 58: Absolute Error = 5.42803815228865e-8, Converged = true, Function Calls = 1315, Gradient Calls = 159, Time = 0.006170034408569336\n",
      "Run 59: Absolute Error = 7.958055103785e-9, Converged = true, Function Calls = 1300, Gradient Calls = 179, Time = 0.00018787384033203125\n",
      "Run 60: Absolute Error = 3.9485223091162425e-8, Converged = true, Function Calls = 640, Gradient Calls = 115, Time = 0.00010585784912109375\n",
      "Run 61: Absolute Error = 1.15592260208075e-8, Converged = true, Function Calls = 1882, Gradient Calls = 183, Time = 0.00023889541625976562\n",
      "Run 62: Absolute Error = 6.105236804998526e-8, Converged = true, Function Calls = 1816, Gradient Calls = 191, Time = 0.00024199485778808594\n",
      "Run 63: Absolute Error = 5.2707462838696983e-8, Converged = true, Function Calls = 1368, Gradient Calls = 153, Time = 0.0001919269561767578\n",
      "Run 64: Absolute Error = 2.9588588024154205e-8, Converged = true, Function Calls = 1274, Gradient Calls = 173, Time = 0.0001850128173828125\n",
      "Run 65: Absolute Error = 2.8229942383717344e-8, Converged = true, Function Calls = 1134, Gradient Calls = 153, Time = 0.000164031982421875\n",
      "Run 66: Absolute Error = 1.3106345253532936e-7, Converged = true, Function Calls = 1236, Gradient Calls = 165, Time = 0.00018310546875\n",
      "Run 67: Absolute Error = 5.103640976500401e-8, Converged = true, Function Calls = 1219, Gradient Calls = 147, Time = 0.00016808509826660156\n",
      "Run 68: Absolute Error = 1.5165469324784908e-8, Converged = true, Function Calls = 1471, Gradient Calls = 185, Time = 0.00020813941955566406\n",
      "Run 69: Absolute Error = 7.410735136659241e-9, Converged = true, Function Calls = 2083, Gradient Calls = 215, Time = 0.0002779960632324219\n",
      "Run 70: Absolute Error = 1.408552252613049e-8, Converged = true, Function Calls = 1333, Gradient Calls = 175, Time = 0.00019288063049316406\n",
      "Run 71: Absolute Error = 8.273896412447357e-8, Converged = true, Function Calls = 1329, Gradient Calls = 149, Time = 0.00017786026000976562\n",
      "Run 72: Absolute Error = 1.4464617947851366e-8, Converged = true, Function Calls = 1389, Gradient Calls = 169, Time = 0.00019288063049316406\n",
      "Run 73: Absolute Error = 3.993294539483827e-8, Converged = true, Function Calls = 1605, Gradient Calls = 187, Time = 0.00022482872009277344\n",
      "Run 74: Absolute Error = 2.375803420662237e-8, Converged = true, Function Calls = 1172, Gradient Calls = 151, Time = 0.0001678466796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 75: Absolute Error = 2.085054884304327e-8, Converged = true, Function Calls = 1775, Gradient Calls = 183, Time = 0.0002319812774658203\n",
      "Run 76: Absolute Error = 5.895660315502482e-8, Converged = true, Function Calls = 1233, Gradient Calls = 155, Time = 0.00017213821411132812\n",
      "Run 77: Absolute Error = 3.002349613367983e-8, Converged = true, Function Calls = 1457, Gradient Calls = 185, Time = 0.00020599365234375\n",
      "Run 78: Absolute Error = 4.389560448458951e-8, Converged = true, Function Calls = 1312, Gradient Calls = 163, Time = 0.00018787384033203125\n",
      "Run 79: Absolute Error = 3.880525767385734e-8, Converged = true, Function Calls = 972, Gradient Calls = 129, Time = 0.00014209747314453125\n",
      "Run 80: Absolute Error = 7.649691502464862e-8, Converged = true, Function Calls = 1987, Gradient Calls = 209, Time = 0.00025916099548339844\n",
      "Run 81: Absolute Error = 7.189246553629403e-8, Converged = true, Function Calls = 1494, Gradient Calls = 183, Time = 0.00020694732666015625\n",
      "Run 82: Absolute Error = 2.3313324160056936e-8, Converged = true, Function Calls = 1324, Gradient Calls = 161, Time = 0.0001850128173828125\n",
      "Run 83: Absolute Error = 3.875835119515614e-8, Converged = true, Function Calls = 1792, Gradient Calls = 207, Time = 0.0002510547637939453\n",
      "Run 84: Absolute Error = 3.0337308665195906e-8, Converged = true, Function Calls = 1445, Gradient Calls = 173, Time = 0.000202178955078125\n",
      "Run 85: Absolute Error = 4.515263718474216e-8, Converged = true, Function Calls = 1060, Gradient Calls = 141, Time = 0.00015401840209960938\n",
      "Run 86: Absolute Error = 5.3766560759171966e-8, Converged = true, Function Calls = 1757, Gradient Calls = 193, Time = 0.00023412704467773438\n",
      "Run 87: Absolute Error = 2.076877203549543e-8, Converged = true, Function Calls = 1103, Gradient Calls = 147, Time = 0.0001621246337890625\n",
      "Run 88: Absolute Error = 2.2344830430398588e-8, Converged = true, Function Calls = 1405, Gradient Calls = 169, Time = 0.0002028942108154297\n",
      "Run 89: Absolute Error = 2.8556160991399793e-8, Converged = true, Function Calls = 1798, Gradient Calls = 189, Time = 0.00023603439331054688\n",
      "Run 90: Absolute Error = 1.1732900428995663e-8, Converged = true, Function Calls = 967, Gradient Calls = 141, Time = 0.00014710426330566406\n",
      "Run 91: Absolute Error = 1.793353110457474e-8, Converged = true, Function Calls = 1138, Gradient Calls = 155, Time = 0.0013949871063232422\n",
      "Run 92: Absolute Error = 2.6935405461614437e-8, Converged = true, Function Calls = 1776, Gradient Calls = 203, Time = 0.0002410411834716797\n",
      "Run 93: Absolute Error = 1.9433637588051056e-8, Converged = true, Function Calls = 1242, Gradient Calls = 159, Time = 0.0001709461212158203\n",
      "Run 94: Absolute Error = 4.5848804308690205e-8, Converged = true, Function Calls = 1533, Gradient Calls = 169, Time = 0.00019884109497070312\n",
      "Run 95: Absolute Error = 1.77833063830235e-8, Converged = true, Function Calls = 1829, Gradient Calls = 187, Time = 0.0002288818359375\n",
      "Run 96: Absolute Error = 5.531266999980744e-9, Converged = true, Function Calls = 1131, Gradient Calls = 141, Time = 0.00015497207641601562\n",
      "Run 97: Absolute Error = 3.418278948785769e-8, Converged = true, Function Calls = 1646, Gradient Calls = 195, Time = 0.0002269744873046875\n",
      "Run 98: Absolute Error = 1.0558094842849641e-8, Converged = true, Function Calls = 1724, Gradient Calls = 187, Time = 0.0002219676971435547\n",
      "Run 99: Absolute Error = 2.6213085035919903e-8, Converged = true, Function Calls = 1505, Gradient Calls = 185, Time = 0.0002040863037109375\n",
      "Run 100: Absolute Error = 1.1592651283365285e-8, Converged = true, Function Calls = 1392, Gradient Calls = 171, Time = 0.0001881122589111328\n",
      "\n",
      "Results for Augmented Lagrangian Method on Booth's function:\n",
      "Run 1: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 1.1762058734893799\n",
      "Run 2: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003650188446044922\n",
      "Run 3: Absolute Error = 5.702341137037652, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003371238708496094\n",
      "Run 4: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003390312194824219\n",
      "Run 5: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003490447998046875\n",
      "Run 6: Absolute Error = 5.702341136096022, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003228187561035156\n",
      "Run 7: Absolute Error = 5.702341136096014, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003209114074707031\n",
      "Run 8: Absolute Error = 5.70234113609602, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.0003058910369873047\n",
      "Run 9: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003180503845214844\n",
      "Run 10: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003337860107421875\n",
      "Run 11: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003249645233154297\n",
      "Run 12: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003190040588378906\n",
      "Run 13: Absolute Error = 5.702341136096012, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003139972686767578\n",
      "Run 14: Absolute Error = 5.70234113609602, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.0003180503845214844\n",
      "Run 15: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00031685829162597656\n",
      "Run 16: Absolute Error = 5.7023411368448, Total Function Calls = 176, Total Gradient Calls = 176, Time = 0.0003139972686767578\n",
      "Run 17: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003249645233154297\n",
      "Run 18: Absolute Error = 5.702341137037652, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00032401084899902344\n",
      "Run 19: Absolute Error = 5.702341137037654, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003170967102050781\n",
      "Run 20: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003199577331542969\n",
      "Run 21: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00033283233642578125\n",
      "Run 22: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003211498260498047\n",
      "Run 23: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003230571746826172\n",
      "Run 24: Absolute Error = 5.702341136844804, Total Function Calls = 177, Total Gradient Calls = 177, Time = 0.0003209114074707031\n",
      "Run 25: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003230571746826172\n",
      "Run 26: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00032401084899902344\n",
      "Run 27: Absolute Error = 5.702341136096017, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.0003139972686767578\n",
      "Run 28: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003361701965332031\n",
      "Run 29: Absolute Error = 5.702341137037652, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00033092498779296875\n",
      "Run 30: Absolute Error = 5.7023411360960194, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00031280517578125\n",
      "Run 31: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003268718719482422\n",
      "Run 32: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00032591819763183594\n",
      "Run 33: Absolute Error = 5.702341136096022, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00030994415283203125\n",
      "Run 34: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00031495094299316406\n",
      "Run 35: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00032401084899902344\n",
      "Run 36: Absolute Error = 5.7023411360960194, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00031304359436035156\n",
      "Run 37: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00031185150146484375\n",
      "Run 38: Absolute Error = 5.70234113609602, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003139972686767578\n",
      "Run 39: Absolute Error = 5.702341136096021, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00030994415283203125\n",
      "Run 40: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003159046173095703\n",
      "Run 41: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00031280517578125\n",
      "Run 42: Absolute Error = 5.702341136096021, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00030303001403808594\n",
      "Run 43: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003101825714111328\n",
      "Run 44: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003159046173095703\n",
      "Run 45: Absolute Error = 5.70234113609602, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.0003190040588378906\n",
      "Run 46: Absolute Error = 5.702341136096013, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.00031304359436035156\n",
      "Run 47: Absolute Error = 5.702341136096018, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003139972686767578\n",
      "Run 48: Absolute Error = 5.702341137037654, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003228187561035156\n",
      "Run 49: Absolute Error = 5.702341136096016, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00030684471130371094\n",
      "Run 50: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00032401084899902344\n",
      "Run 51: Absolute Error = 5.702341137037652, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00032401084899902344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 52: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003170967102050781\n",
      "Run 53: Absolute Error = 5.70234113609602, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00031113624572753906\n",
      "Run 54: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003190040588378906\n",
      "Run 55: Absolute Error = 5.702341137037655, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0003180503845214844\n",
      "Run 56: Absolute Error = 5.702341137037654, Total Function Calls = 178, Total Gradient Calls = 178, Time = 0.00031685829162597656\n",
      "Run 57: Absolute Error = 5.702341136096021, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00031495094299316406\n",
      "Run 58: Absolute Error = 5.702341136096018, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00030517578125\n",
      "Run 59: Absolute Error = 5.702341136096012, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.0003070831298828125\n",
      "Run 60: Absolute Error = 5.702341136096016, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.0003058910369873047\n",
      "Run 61: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00031304359436035156\n",
      "Run 62: Absolute Error = 5.7023411360960194, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00030303001403808594\n",
      "Run 63: Absolute Error = 5.70234113609602, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00030612945556640625\n",
      "Run 64: Absolute Error = 5.702341137037654, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0003159046173095703\n",
      "Run 65: Absolute Error = 5.702341137037652, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00031185150146484375\n",
      "Run 66: Absolute Error = 5.702341136096019, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.009831905364990234\n",
      "Run 67: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00028896331787109375\n",
      "Run 68: Absolute Error = 5.702341136096019, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00028204917907714844\n",
      "Run 69: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0002980232238769531\n",
      "Run 70: Absolute Error = 5.702341136096022, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.0002810955047607422\n",
      "Run 71: Absolute Error = 5.702341137037654, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0002970695495605469\n",
      "Run 72: Absolute Error = 5.702341137037652, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.00029921531677246094\n",
      "Run 73: Absolute Error = 5.702341136096021, Total Function Calls = 172, Total Gradient Calls = 172, Time = 0.0002830028533935547\n",
      "Run 74: Absolute Error = 5.702341137037654, Total Function Calls = 178, Total Gradient Calls = 178, Time = 0.00029206275939941406\n",
      "Run 75: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003070831298828125\n",
      "Run 76: Absolute Error = 5.702341136096013, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00028204917907714844\n",
      "Run 77: Absolute Error = 5.702341136096021, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.00028705596923828125\n",
      "Run 78: Absolute Error = 5.702341137037654, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0002989768981933594\n",
      "Run 79: Absolute Error = 5.702341136096014, Total Function Calls = 174, Total Gradient Calls = 174, Time = 0.00029397010803222656\n",
      "Run 80: Absolute Error = 5.702341137037652, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0002942085266113281\n",
      "Run 81: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00029397010803222656\n",
      "Run 82: Absolute Error = 5.702341137037652, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00030303001403808594\n",
      "Run 83: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0002911090850830078\n",
      "Run 84: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0002961158752441406\n",
      "Run 85: Absolute Error = 5.70234113609602, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.0002899169921875\n",
      "Run 86: Absolute Error = 5.702341137037653, Total Function Calls = 180, Total Gradient Calls = 180, Time = 0.0002968311309814453\n",
      "Run 87: Absolute Error = 5.702341136096021, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.0002841949462890625\n",
      "Run 88: Absolute Error = 5.702341137037654, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.00029206275939941406\n",
      "Run 89: Absolute Error = 5.7023411360960115, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.0003459453582763672\n",
      "Run 90: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0002949237823486328\n",
      "Run 91: Absolute Error = 5.702341136096013, Total Function Calls = 175, Total Gradient Calls = 175, Time = 0.00028514862060546875\n",
      "Run 92: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0002980232238769531\n",
      "Run 93: Absolute Error = 5.702341137037653, Total Function Calls = 181, Total Gradient Calls = 181, Time = 0.0002980232238769531\n",
      "Run 94: Absolute Error = 5.702341136844798, Total Function Calls = 178, Total Gradient Calls = 178, Time = 0.0002930164337158203\n",
      "Run 95: Absolute Error = 5.702341136096017, Total Function Calls = 173, Total Gradient Calls = 173, Time = 0.0002868175506591797\n",
      "Run 96: Absolute Error = 5.702341137037653, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003070831298828125\n",
      "Run 97: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.0003018379211425781\n",
      "Run 98: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00029778480529785156\n",
      "Run 99: Absolute Error = 5.702341137037654, Total Function Calls = 178, Total Gradient Calls = 178, Time = 0.00030803680419921875\n",
      "Run 100: Absolute Error = 5.702341137037654, Total Function Calls = 179, Total Gradient Calls = 179, Time = 0.00030303001403808594\n"
     ]
    }
   ],
   "source": [
    "# Generate initial points for Booth's function\n",
    "initial_points_booths = generate_initial_points()  # Using multiple samples\n",
    "\n",
    "# Get the analytical solution using LBFGS (for comparison)\n",
    "booths_opt_res = optimize(booths_n, convert(Vector{Float64}, initial_points_booths[1]), LBFGS(); autodiff=:forward)\n",
    "booths_opt_min_x = Optim.minimizer(booths_opt_res)\n",
    "\n",
    "# Lists to store results\n",
    "results_conj_booths = []\n",
    "results_aug_lag_booths = []\n",
    "\n",
    "# Run the optimizer for each method\n",
    "for (method_name, results) in zip([\"Conjugate Gradient Descent\", \"Augmented Lagrangian Method\"], [results_conj_booths, results_aug_lag_booths])\n",
    "    for x0 in initial_points_booths\n",
    "        x0 = convert(Vector{Float64}, x0)\n",
    "        start_time = time()\n",
    "        if method_name == \"Conjugate Gradient Descent\"\n",
    "            # Initialize CGD\n",
    "            M = ConjugateGradientDescent(zeros(length(x0)), zeros(length(x0)), 0, 0, 0, false)\n",
    "            init!(M, booths_n, booths_n_gradient, x0)\n",
    "            iteration_limit = 1000\n",
    "            x = x0\n",
    "            for i in 1:iteration_limit\n",
    "                x = step!(M, booths_n, booths_n_gradient, x)\n",
    "                if M.converged\n",
    "                    break\n",
    "                end\n",
    "                if any(isnan, x) || any(isinf, x)\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            elapsed_time = time() - start_time\n",
    "            f_calls, g_calls, iterations, converged = M.func_calls, M.grad_calls, M.iterations, M.converged\n",
    "            absolute_error = norm(x - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, converged, f_calls, g_calls, elapsed_time))\n",
    "        elseif method_name == \"Augmented Lagrangian Method\"\n",
    "            # Use augmented Lagrangian method\n",
    "            k_max = 10  # Maximum number of outer iterations\n",
    "            x_opt, total_f_calls, total_g_calls = augmented_lagrange_method(booths_n, h, x0, k_max)\n",
    "            elapsed_time = time() - start_time\n",
    "            absolute_error = norm(x_opt - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, total_f_calls, total_g_calls, elapsed_time))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Print results for Conjugate Gradient Descent\n",
    "println(\"Results for Conjugate Gradient Descent on Booth's function:\")\n",
    "for (i, res) in enumerate(results_conj_booths)\n",
    "    println(\"Run $i: Absolute Error = $(res[1]), Converged = $(res[2]), Function Calls = $(res[3]), Gradient Calls = $(res[4]), Time = $(res[5])\")\n",
    "end\n",
    "\n",
    "# Print results for Augmented Lagrangian Method\n",
    "println(\"\\nResults for Augmented Lagrangian Method on Booth's function:\")\n",
    "for (i, res) in enumerate(results_aug_lag_booths)\n",
    "    println(\"Run $i: Absolute Error = $(res[1]), Total Function Calls = $(res[2]), Total Gradient Calls = $(res[3]), Time = $(res[4])\")\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
