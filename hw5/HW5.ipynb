{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa9b34f",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "Alles Rebel\n",
    "Computational Science PhD\n",
    "\n",
    "\n",
    "\n",
    "## Intoduction!\n",
    "This homework looks at constrained optimization. We'll explore implementing the algorithms descibed in K&W and see how they work on real hardware! We'll collect data and discuss any interesting bits. \n",
    "\n",
    "Didn't attempt the extra credit - just focused on getting the minimum done for a grad student.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff63a6",
   "metadata": {},
   "source": [
    "## Pre-Reqs\n",
    "\n",
    "Like other homeworks, we'll start by getting the kernel set up with the right dependencies. Running this once should do the trick for the rest of the homework;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4deea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"PrettyTables\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"ForwardDiff\")\n",
    "Pkg.add(\"Optim\")\n",
    "\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using Random\n",
    "using Distributions\n",
    "using Statistics\n",
    "using Dates\n",
    "using PrettyTables\n",
    "using Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf590a",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "The goal of this assignment is to apply a gradient solver to a constrainted optimization problem. In this case the problem needs to be atleast 10 dimensions. The method I'll stick with is the first one suggested by the assignment: the method of Augmented Lagrangian! And we'll apply it to Booth's Function, just updated to use 10 variables instead of 2 via the chaining method introduced in HW2, 1C.\n",
    "\n",
    "### Broad Overview of Augmented Lagrangians\n",
    "In general, these methods solve optimization problems with constraints by incorporating the constraints as additional terms in the objective function. These additional terms involve new parameters, known as Lagrange multipliers, which help reformulate the constrained problem (involving equalities or inequalities) into an unconstrained one.\n",
    "\n",
    "To ensure numerical stablity - Augmented lagrangians intoduce a penality term to the discourage constraint violations. The optimization alternates between minimizing the augmented Lagrangian and updating the multipliers and penalty parameters iteratively. That's what we'll be implementing below!\n",
    "\n",
    "### Booth's Function!\n",
    "\n",
    "We'll reuse the code I wrote from the earlier homework - line for line. Originally designed to be used with existing autodiff (forwardiff in julia) and Optim package. It will suit our purposes for this homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53e0ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Booth's function - generalized for any number of variables\n",
    "# Uses the method suggested by Homework2 Problem 1C to chain variables\n",
    "# pair wise to extend from 2D -> ND\n",
    "# I wrote this for HW2\n",
    "function booths_n(x; n = 10)\n",
    "    result = 0.0\n",
    "    # same rationale for ending at n-1, we need the nth element for n-1\n",
    "    for i in 1:(n - 1)\n",
    "        result += ((x[i] + 2*x[i+1] - 7)^2 + (2*x[i] + x[i+1] - 5)^2)\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "# Same with this - since the methods of HW needed gradients\n",
    "# we'll need this as well for this HW\n",
    "function booths_n_gradient(x; n = length(x))\n",
    "    grad = zeros(length(x)) # modified to be typeless\n",
    "    \n",
    "    # Special case for the first element\n",
    "    grad[1] = 2*(x[1] + 2*x[2] - 7) + 4*(2*x[1] + x[2] - 5)\n",
    "    \n",
    "    # Compute the gradient for the 2:n-1 elements\n",
    "    for i in 2:(n-1)\n",
    "        grad[i] = 2*(x[i]+2*x[i+1]-7) + 4*(2*x[i] + x[i+1] - 5) + 4*(x[i-1] + 2*x[i] - 7) + 2*(2*x[i-1] + x[i] - 5)\n",
    "    end\n",
    "\n",
    "    # Special case for the last element (n-th element)\n",
    "    grad[n] = 4*(x[n-1] + 2*x[n] - 7) + 2*(2*x[n-1] + x[n] - 5) \n",
    "    \n",
    "    return grad\n",
    "end\n",
    "\n",
    "# since this is a constraints optimization problem, we'll add something here\n",
    "\n",
    "# Define the constraint h(x)\n",
    "function h(x)\n",
    "    return [sum(x)]  # For example, we can constrain the sum of x to zero\n",
    "end    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9fb0d",
   "metadata": {},
   "source": [
    "### Implemention of augmented lagrangian method\n",
    "\n",
    "This will be a bit involved, but in general - it's the following steps:\n",
    "- We'll attempt to use Julia's Optim library for the minimization method\n",
    "- We'll instrument up the result to match the results from earlier\n",
    "(function calls, grad calls, convergement measure, optimization point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1dccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augmented_lagrange_method (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Augmented Lagrangian Method using Optim.optimize\n",
    "# Based upon the code in Algorithms for Optimization (Algorithm 10.2 of K&W)\n",
    "# Modified to keep track of function and grad calls\n",
    "function augmented_lagrange_method(f, h, x0, k_max; ρ=1.0, γ=2.0, tol=1e-6)\n",
    "    x = x0\n",
    "    λ = zeros(length(h(x)))               # Initialize Lagrange multipliers\n",
    "    old_f_value = f(x)                    # Track old objective value\n",
    "    convergence_measure = 0.0             # Convergence measure\n",
    "\n",
    "    total_f_calls = 0   # Total function evaluations\n",
    "    total_g_calls = 0   # Total gradient evaluations\n",
    "\n",
    "    for k in 1:k_max\n",
    "        # Define penalty function\n",
    "        function p(x)\n",
    "            λ_promoted = convert.(eltype(x[1]), λ)\n",
    "            return (ρ / 2) * sum(h(x).^2) - dot(λ_promoted, h(x))\n",
    "        end\n",
    "\n",
    "        # Define augmented objective function\n",
    "        function augmented_objective(x)\n",
    "            return f(x) + p(x)\n",
    "        end\n",
    "\n",
    "        # Optimize using Optim.optimize\n",
    "        result = optimize(augmented_objective, x, method=LBFGS(); autodiff=:forward)\n",
    "        x = Optim.minimizer(result)\n",
    "\n",
    "        # Accumulate function and gradient evaluations\n",
    "        total_f_calls += result.f_calls\n",
    "        total_g_calls += result.g_calls\n",
    "\n",
    "        # Update convergence metrics\n",
    "        new_f_value = f(x)\n",
    "        convergence_measure = abs(new_f_value - old_f_value)\n",
    "        old_f_value = new_f_value\n",
    "\n",
    "        # Check for convergence\n",
    "        if convergence_measure < tol\n",
    "            break\n",
    "        end\n",
    "\n",
    "        # Update Lagrange multipliers and penalty parameter\n",
    "        λ .= λ .- ρ * ForwardDiff.value.(h(x))\n",
    "        ρ *= γ\n",
    "    end\n",
    "\n",
    "    return x, total_f_calls, total_g_calls\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7042e",
   "metadata": {},
   "source": [
    "### Experimental Methodology\n",
    "\n",
    "We'll utilize the same testing methodology as the HW2, generating a BUNCH of random vectors to start from. And then running the method across each of the vectors, recording how many function calls, gradient calls, exectuation time and error from what the ideal solution would have been. We'll compare the above method against the earlier line search method.\n",
    "\n",
    "First we'll get the environment ready to do the experiment - how we'll generate everything an calculate the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e4eb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code we used earlier in HW2! This will be what we'll compare against\n",
    "# I wrote this code with the book as the primary sources\n",
    "# Conjugate Gradient Descent method (with function and gradient counts)\n",
    "# Updated tracking - and fixed accounting issues missed in originial HW2 implementation\n",
    "mutable struct ConjugateGradientDescent\n",
    "    d::Vector{Float64}\n",
    "    g::Vector{Float64}\n",
    "    iterations::Int\n",
    "    func_calls::Int\n",
    "    grad_calls::Int\n",
    "    converged::Bool\n",
    "end\n",
    "\n",
    "function init!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    M.g = ∇f(x)\n",
    "    M.d = -M.g\n",
    "    M.iterations = 0\n",
    "    M.func_calls = 1\n",
    "    M.grad_calls = 1\n",
    "    M.converged = false\n",
    "    return M\n",
    "end\n",
    "\n",
    "function backtracking_line_search(f, ∇f, x, d, α; p=0.5, β=1e-4)\n",
    "    y, g = f(x), ∇f(x)\n",
    "    func_calls = 1\n",
    "    grad_calls = 1\n",
    "    while f(x + α*d) > y + β*α*(g⋅d)\n",
    "        α *= p\n",
    "        func_calls += 1\n",
    "    end\n",
    "    return α, func_calls, grad_calls\n",
    "end\n",
    "\n",
    "function step!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    d, g = M.d, M.g\n",
    "    α, f_calls_ls, g_calls_ls = backtracking_line_search(f, ∇f, x, d, 1.0)  # initial α can be 1.0\n",
    "    x_new = x + α * d\n",
    "    M.func_calls += f_calls_ls\n",
    "    M.grad_calls += g_calls_ls\n",
    "    M.func_calls += 1  # For evaluating f at x_new\n",
    "    g_new = ∇f(x_new)\n",
    "    M.grad_calls += 1\n",
    "    β = max(0, dot(g_new, g_new - g) / dot(g, g))\n",
    "    d_new = -g_new + β * d\n",
    "    M.d = d_new\n",
    "    M.g = g_new\n",
    "    M.iterations += 1\n",
    "    if norm(g_new) < 1e-6\n",
    "        M.converged = true\n",
    "    end\n",
    "    return x_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5438eb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_initial_points (generic function with 5 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The code I wrote to generate the starting points from HW2\n",
    "function generate_initial_points(left_limit=-10.0, right_limit=10.0, samples=100, dims=10)\n",
    "    # Generate 100 random real-valued starting points as specified\n",
    "    initial_points = [] \n",
    "\n",
    "    for i in 1:samples # defaults to 100 samples\n",
    "        this_sample = []\n",
    "        \n",
    "        for j in 1:dims\n",
    "            # y is uniformly picked in [-10.0, 10.0] by default\n",
    "            # note, rand is already uniform, just need to cap the limits\n",
    "            y = rand(Uniform(left_limit, right_limit))\n",
    "            # choose + or - with equal probability, we'll use bitrand (50/50)\n",
    "            sign = bitrand() == 0 ? -1 : 1\n",
    "            x = sign * exp(y)\n",
    "            push!(this_sample, x)\n",
    "        end\n",
    "        \n",
    "        push!(initial_points, this_sample)\n",
    "    end\n",
    "\n",
    "    return initial_points\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdb955",
   "metadata": {},
   "source": [
    "Running the experiment is similiar to how we did it before - we generate 100 random sample starting points. And for each, we run both methods to see how well the methods perform in terms of time and function/gradient calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa560e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment code is effectively the same as previous homeworks with renamed variables\n",
    "# Handles the time keeping of each optimizization method\n",
    "\n",
    "# Generate initial points for Booth's function\n",
    "initial_points_booths = generate_initial_points()  # Using multiple samples, 100 as we did before\n",
    "\n",
    "# Get the numberical solution using LBFGS (for comparison), using built in solver in Optim\n",
    "booths_opt_res = optimize(booths_n, convert(Vector{Float64}, initial_points_booths[1]), LBFGS(); autodiff=:forward)\n",
    "booths_opt_min_x = Optim.minimizer(booths_opt_res)\n",
    "\n",
    "# Lists to store results\n",
    "results_conj_booths = []\n",
    "results_aug_lag_booths = []\n",
    "\n",
    "# Run the optimizer for each method\n",
    "for (method_name, results) in zip([\"Conjugate Gradient Descent\", \"Augmented Lagrangian Method\"], [results_conj_booths, results_aug_lag_booths])\n",
    "    for x0 in initial_points_booths\n",
    "        x0 = convert(Vector{Float64}, x0)\n",
    "        start_time = time()\n",
    "        if method_name == \"Conjugate Gradient Descent\"\n",
    "            # Initialize CGD\n",
    "            M = ConjugateGradientDescent(zeros(length(x0)), zeros(length(x0)), 0, 0, 0, false)\n",
    "            init!(M, booths_n, booths_n_gradient, x0)\n",
    "            iteration_limit = 1000\n",
    "            x = x0\n",
    "            for i in 1:iteration_limit\n",
    "                x = step!(M, booths_n, booths_n_gradient, x)\n",
    "                if M.converged\n",
    "                    break\n",
    "                end\n",
    "                if any(isnan, x) || any(isinf, x)\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            elapsed_time = time() - start_time\n",
    "            f_calls, g_calls, iterations, converged = M.func_calls, M.grad_calls, M.iterations, M.converged\n",
    "            absolute_error = norm(x - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, converged, f_calls, g_calls, elapsed_time))\n",
    "        elseif method_name == \"Augmented Lagrangian Method\"\n",
    "            # Use augmented Lagrangian method\n",
    "            k_max = 10  # Maximum number of outer iterations\n",
    "            x_opt, total_f_calls, total_g_calls = augmented_lagrange_method(booths_n, h, x0, k_max)\n",
    "            elapsed_time = time() - start_time\n",
    "            absolute_error = norm(x_opt - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, total_f_calls, total_g_calls, elapsed_time))\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c367c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOptimization Results (mean ± std)\u001b[0m\n",
      "┌─────────────────────────────┬─────────────────────────┬──────────────────┬───────────────┐\n",
      "│\u001b[1m                      Method \u001b[0m│\u001b[1m          Absolute Error \u001b[0m│\u001b[1m      Evaluations \u001b[0m│\u001b[1m     Time (ms) \u001b[0m│\n",
      "├─────────────────────────────┼─────────────────────────┼──────────────────┼───────────────┤\n",
      "│  Conjugate Gradient Descent │ 4.2703e-08 ± 3.0150e-08 │ 1388.95 ± 325.87 │   1.00 ± 0.00 │\n",
      "│ Augmented Lagrangian Method │ 5.7023e+00 ± 4.5574e-10 │    177.29 ± 3.11 │ 177.29 ± 3.11 │\n",
      "└─────────────────────────────┴─────────────────────────┴──────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Function to compute statistics and display results in a table\n",
    "# Adapted from previous homeworks (using PrettyTable to show the info!)\n",
    "function display_results_table(method_names, results_list)\n",
    "    # Collect data rows\n",
    "    data_rows = []\n",
    "\n",
    "    for (method_name, results) in zip(method_names, results_list)\n",
    "        errors = [res[1] for res in results]\n",
    "        times = [res[2] for res in results]\n",
    "        evals = [res[3] for res in results]\n",
    "\n",
    "        mean_error = mean(errors)\n",
    "        std_error = std(errors)\n",
    "        mean_time = mean(times)\n",
    "        std_time = std(times)\n",
    "        mean_evals = mean(evals)\n",
    "        std_evals = std(evals)\n",
    "\n",
    "        # Create data row using hcat\n",
    "        data_row = hcat(\n",
    "            method_name,\n",
    "            @sprintf(\"%.4e ± %.4e\", mean_error, std_error),\n",
    "            @sprintf(\"%.2f ± %.2f\", mean_evals, std_evals),\n",
    "            @sprintf(\"%.2f ± %.2f\", mean_time, std_time)\n",
    "        )\n",
    "\n",
    "        push!(data_rows, data_row)\n",
    "    end\n",
    "\n",
    "    # Combine data rows using vcat\n",
    "    table = vcat(data_rows...)\n",
    "\n",
    "    # Display the table using PrettyTables\n",
    "    headers = [\"Method\", \"Absolute Error\", \"Evaluations\", \"Time (ms)\"]\n",
    "    pretty_table(table, header = headers, title = \"Optimization Results (mean ± std)\")\n",
    "end\n",
    "\n",
    "# Prepare the data for displaying\n",
    "method_names = [\"Conjugate Gradient Descent\", \"Augmented Lagrangian Method\"]\n",
    "results_list = [results_conj_booths, results_aug_lag_booths]\n",
    "\n",
    "# Call the function to display the results table\n",
    "display_results_table(method_names, results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec8f691",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "I suspect there's a bug in the augmented lagragian implementation - when combing through a few runs by printing the results - it seems that Augmented Lagrangian tends to finish faster than Conjugate Gradient; there's a bunch of runs that end up taking a long time due to poor line searching. However, overall function and gradient evaltions are significantly lower in Augmented Lagrangian vs Conjugate Gradient Descent. Another interesting bit of information is the error - the implementation error results in the absolute error being quite large. If more time was available, I would have dug into this more.\n",
    "\n",
    "Another interesting point is that the Conjugate Gradient Descent didn't take into account the constraint! So it's possible Agumented Lagrangian is not only faster - but more accurate given the made up constraint I put in. Since they are optimizing different functions - the evaluations make a bit more sense to use rather than time or error. I could have also selected a better constraint.\n",
    "\n",
    "Note: the absolute error isn't also useful either - it doesn't take into account the constraint! So really, it's just evaluations and timing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47170e6",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "This problem we're going to use a stocastic solver, simulated annealing, on the Booth function! Atleast this time we can compare the two methods head to head, both will operate on the same constraints. Very simliar experimental set up as Problem 1. \n",
    "\n",
    "We'll utilize http://julianlsolvers.github.io/Optim.jl/stable/algo/simulated_annealing/ for most of the information. Our focus will be to get it instrumented as much as we can, and collect some stats!\n",
    "\n",
    "The goal will be to apply it against a 10 variable Booth's equation. And for fun, we'll compare it against the Augmented Lagrangian method from Problem 1.\n",
    "\n",
    "### Experiment\n",
    "\n",
    "It'll be the same drill as before - collecting 100 random starting points and running through those for each method. We'll reuse as much code as possible from previous problem and homeworks. We'll compare against the solution created by using the built in LBFGS optimizer in Optim. Same as before regarding the conversion of 2 Variable booths to 10 variable booths (via variable chaining by pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbd4733",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOptimization Results (mean ± std)\u001b[0m\n",
      "┌─────────────────────────────┬─────────────────────────┬────────────────┬──────────────────┐\n",
      "│\u001b[1m                      Method \u001b[0m│\u001b[1m          Absolute Error \u001b[0m│\u001b[1m    Evaluations \u001b[0m│\u001b[1m        Time (ms) \u001b[0m│\n",
      "├─────────────────────────────┼─────────────────────────┼────────────────┼──────────────────┤\n",
      "│         Simulated Annealing │ 7.5706e+03 ± 6.1135e+03 │ 1001.00 ± 0.00 │ 3.3462 ± 31.2140 │\n",
      "│ Augmented Lagrangian Method │ 5.7023e+00 ± 4.5883e-10 │  354.42 ± 6.15 │  0.3258 ± 0.0164 │\n",
      "└─────────────────────────────┴─────────────────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "## Experiment code is effectively the same as previous homeworks with renamed variables\n",
    "# Handles the time keeping of each optimizization method\n",
    "\n",
    "# Generate initial points for Booth's function\n",
    "initial_points_booths = generate_initial_points()  # Using multiple samples, 100 as we did before\n",
    "\n",
    "# Get the numberical solution using LBFGS (for comparison), using built in solver in Optim\n",
    "booths_opt_res = optimize(booths_n, convert(Vector{Float64}, initial_points_booths[1]), LBFGS(); autodiff=:forward)\n",
    "booths_opt_min_x = Optim.minimizer(booths_opt_res)\n",
    "\n",
    "# Lists to store results\n",
    "results_sim_ann_booths = []\n",
    "results_aug_lag_booths = []\n",
    "\n",
    "# Run the optimizer for each method\n",
    "for (method_name, results) in zip([\"Simulated Annealing\", \"Augmented Lagrangian Method\"], [results_sim_ann_booths, results_aug_lag_booths])\n",
    "    for x0 in initial_points_booths\n",
    "        x0 = convert(Vector{Float64}, x0)\n",
    "        start_time = time()\n",
    "        if method_name == \"Simulated Annealing\"\n",
    "            # Use Optim.optimize with SimulatedAnnealing\n",
    "            result = optimize(booths_n, x0, method=SimulatedAnnealing())\n",
    "            x = Optim.minimizer(result)\n",
    "            elapsed_time = time() - start_time\n",
    "            f_calls = result.f_calls\n",
    "            total_evals = f_calls\n",
    "            absolute_error = norm(x - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, total_evals, elapsed_time*1e3))\n",
    "        elseif method_name == \"Augmented Lagrangian Method\"\n",
    "            # Use augmented Lagrangian method\n",
    "            k_max = 10  # Maximum number of outer iterations\n",
    "            x_opt, total_f_calls, total_g_calls = augmented_lagrange_method(booths_n, h, x0, k_max)\n",
    "            elapsed_time = time() - start_time\n",
    "            total_evals = total_f_calls + total_g_calls\n",
    "            absolute_error = norm(x_opt - booths_opt_min_x, Inf)\n",
    "            push!(results, (absolute_error, total_evals, elapsed_time*1e3))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to compute statistics and display results in a table\n",
    "function display_results_table(method_names, results_list)\n",
    "    # Collect data rows\n",
    "    data_rows = []\n",
    "    for (method_name, results) in zip(method_names, results_list)\n",
    "        errors = [res[1] for res in results]\n",
    "        evals = [res[2] for res in results]\n",
    "        times = [res[3] for res in results]\n",
    "        mean_error = mean(errors)\n",
    "        std_error = std(errors)\n",
    "        mean_evals = mean(evals)\n",
    "        std_evals = std(evals)\n",
    "        mean_time = mean(times)\n",
    "        std_time = std(times)\n",
    "        # Create data row\n",
    "        data_row = hcat(\n",
    "            method_name,\n",
    "            @sprintf(\"%.4e ± %.4e\", mean_error, std_error),\n",
    "            @sprintf(\"%.2f ± %.2f\", mean_evals, std_evals),\n",
    "            @sprintf(\"%.4f ± %.4f\", mean_time, std_time)\n",
    "        )\n",
    "        push!(data_rows, data_row)\n",
    "    end\n",
    "    # Combine data rows using vcat\n",
    "    table = vcat(data_rows...)\n",
    "    # Display the table using PrettyTables\n",
    "    headers = [\"Method\", \"Absolute Error\", \"Evaluations\", \"Time (ms)\"]\n",
    "    pretty_table(table, header = headers, title = \"Optimization Results (mean ± std)\")\n",
    "end\n",
    "\n",
    "# Prepare the data for displaying\n",
    "method_names = [\"Simulated Annealing\", \"Augmented Lagrangian Method\"]\n",
    "results_list = [results_sim_ann_booths, results_aug_lag_booths]\n",
    "\n",
    "# Call the function to display the results table\n",
    "display_results_table(method_names, results_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a695d",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "To be honest, based on the number of evaluations - I would have expected Simulated Annealing to take MUCH longer. It's possible it's using the same evaluations over and over, meaning an evaluation's result may be a 'hot' cache item - causing really quick fetching. This makes sense in the context of simulated annealing - it's supposed to perfer the more likely outcomes - and this would be the definition of a cache friendly operation. However, this would require further investigation to verify.\n",
    "\n",
    "Augmented Lagrangian performs the same as the previous method (which it should! no implementation changes). It's atleast consistent with similiar timings and error. The error is actually much larger with Simulated Annealing. But I don't trust the error much given that the solution found by the optimizer doesn't take into account my made up constraint. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
