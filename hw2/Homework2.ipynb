{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df46e30c",
   "metadata": {},
   "source": [
    "# Homework \\#2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e180f15",
   "metadata": {},
   "source": [
    "Alles Rebel\n",
    "\n",
    "Computional Science PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ceb32",
   "metadata": {},
   "source": [
    "## Julia Package Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96a009",
   "metadata": {},
   "source": [
    "Run these cells before anything else just to ensure the julia environment has all the dependencies it needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ad8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"PrettyTables\")\n",
    "Pkg.add(\"Optim\")\n",
    "\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "using Random\n",
    "using Distributions\n",
    "using Statistics\n",
    "using Dates\n",
    "using PrettyTables\n",
    "using Optim\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4251ef",
   "metadata": {},
   "source": [
    "## Problem 1, Multi-Variable Unconstrained Optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5d1e4",
   "metadata": {},
   "source": [
    "Problem 1 is all about gradient based optimization. We'll have access to derivatives staying at first order only. We'll look at two methods, a regular gradient descent method and a conjugate gradient descent method. In both cases we'll instrument and compare them!\n",
    "\n",
    "Similiar the the previous homeworks, we'll be using the following template from K&W algorithm 5.1 for gradabilty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645f2333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Original K&W algorithm 5.1 Code! \n",
    "# It's a functional gradient descent optimizer... but we'll add stuff soon\n",
    "abstract type DescentMethod end\n",
    "\n",
    "struct GradientDescent <: DescentMethod\n",
    "    α\n",
    "end\n",
    "\n",
    "init!(M::GradientDescent, f, ∇f, x) = M\n",
    "\n",
    "function step!(M::GradientDescent, f, ∇f, x)\n",
    "    α, g = M.α, ∇f(x)\n",
    "    return x - α*g\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abfd23",
   "metadata": {},
   "source": [
    "### Problem 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3193cc",
   "metadata": {},
   "source": [
    "#### Quick Code Explaination!\n",
    "The template code is actually a functional implementation!\n",
    "A quick run down: $DescentMethod$ defines a interface. A immutable struct is declared, that inherits the DescentMethod interface, including the parameters that are needed for specifically $GradidentDescent$. The struct contains a single value, a $\\alpha$ learning rate. \n",
    "\n",
    "As mentioned by the text, there's an inline function called $init!$. It's a function doesn't actually do anything meaningful - all it does is take the passed in parameter $M$ and return it. It does specify that the parameter should be a instance of $GradientDescent$ but beyond that, it's not super useful (yet).\n",
    "\n",
    "The meat and bones in $step!$, which is where we'll be implementing the actual logic for the gradient descent. The next value of $x$ is returned, subtracting off the gradient direction times the learning rate defined in the $M$ parameter passed in. An example usage is shown below of the template code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615e5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = 0.6000000, f(x) = 5.7600000\n",
      "Iteration 2: x = 1.0800000, f(x) = 3.6864000\n",
      "Iteration 3: x = 1.4640000, f(x) = 2.3592960\n",
      "Iteration 4: x = 1.7712000, f(x) = 1.5099494\n",
      "Iteration 5: x = 2.0169600, f(x) = 0.9663676\n",
      "Iteration 6: x = 2.2135680, f(x) = 0.6184753\n",
      "Iteration 7: x = 2.3708544, f(x) = 0.3958242\n",
      "Iteration 8: x = 2.4966835, f(x) = 0.2533275\n",
      "Iteration 9: x = 2.5973468, f(x) = 0.1621296\n",
      "Iteration 10: x = 2.6778775, f(x) = 0.1037629\n",
      "Iteration 11: x = 2.7423020, f(x) = 0.0664083\n",
      "Iteration 12: x = 2.7938416, f(x) = 0.0425013\n",
      "Iteration 13: x = 2.8350733, f(x) = 0.0272008\n",
      "Iteration 14: x = 2.8680586, f(x) = 0.0174085\n",
      "Iteration 15: x = 2.8944469, f(x) = 0.0111415\n",
      "Iteration 16: x = 2.9155575, f(x) = 0.0071305\n",
      "Iteration 17: x = 2.9324460, f(x) = 0.0045635\n",
      "Iteration 18: x = 2.9459568, f(x) = 0.0029207\n",
      "Iteration 19: x = 2.9567654, f(x) = 0.0018692\n",
      "Iteration 20: x = 2.9654124, f(x) = 0.0011963\n",
      "Found solution: x ≈ 2.9654124\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the K&W algorithm 5.1 Code! \n",
    "f(x) = (x - 3)^2 # an example! root = 3\n",
    "∇f(x) = 2 * (x - 3) # the derivative\n",
    "\n",
    "M = GradientDescent(0.1)\n",
    "x = 0.0  # Initial guess, we'll make this random soon\n",
    "\n",
    "for i in 1:20\n",
    "    x = step!(M, f, ∇f, x)\n",
    "    @printf(\"Iteration %d: x = %2.7f, f(x) = %2.7f\\n\", i, x, f(x))\n",
    "end\n",
    "\n",
    "@printf(\"Found solution: x ≈ %2.7f\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748538f0",
   "metadata": {},
   "source": [
    "#### Enhancements...\n",
    "\n",
    "We'll want to keep track of a few things:\n",
    "* How many function calls/Number of Iterations?\n",
    "* Tolerance? Maybe we get to the solution early! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c48920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_stats (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Updated K&W algorithm 5.1 Code! \n",
    "# Everything else is the same, just updated the step! function with basic instrumentation\n",
    "# Enhanced with function call instrumentation + early exit based on tolerance\n",
    "# the interface is the same as other DescentMethods\n",
    "mutable struct MyGradientDescent <: DescentMethod\n",
    "    α::Float64\n",
    "\n",
    "    # instrumention stuff\n",
    "    func_calls::Int\n",
    "    grad_calls::Int\n",
    "    iterations::Int\n",
    "    converged::Float64\n",
    "\n",
    "    # Constructor with optional instrumentation fields defaulting to zero/false\n",
    "    function MyGradientDescent(α::Float64)\n",
    "        new(α, 0, 0, 0, 0.0)\n",
    "    end\n",
    "end\n",
    "\n",
    "function init!(M::MyGradientDescent, f, ∇f, x)\n",
    "    # instrumentation init, since we want to reuse the M\n",
    "    M.func_calls = 0\n",
    "    M.grad_calls = 0\n",
    "    M.iterations = 0\n",
    "    M.converged = 0.0\n",
    "end\n",
    "\n",
    "function step!(M::MyGradientDescent, f, ∇f, x; tol=1e-6)\n",
    "    # early exit check\n",
    "    if M.converged > 0.0\n",
    "        return x\n",
    "    end\n",
    "    \n",
    "    # we're about to actually do an iteration/step\n",
    "    M.iterations += 1\n",
    "    \n",
    "    α, g = M.α, ∇f(x)\n",
    "    M.grad_calls += 1\n",
    "    \n",
    "    # tolerance check, maybe we're done! If the magnitude / norm of the gradient is small\n",
    "    # we're probably in the minima (or saddle point\n",
    "    if( norm(g) < tol )\n",
    "        M.converged = norm(g)\n",
    "    end\n",
    "\n",
    "    return x - α * g\n",
    "end\n",
    "\n",
    "# one liner stats call\n",
    "get_stats(M::MyGradientDescent) = M.func_calls, M.grad_calls, M.iterations, M.converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d6ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found solution: x ≈ 2.9999996\n",
      "Metrics: 0 function calls, 71 grad calls, 0.0000010 error in 71 iterations \n"
     ]
    }
   ],
   "source": [
    "# Example usage of the instrumented Version\n",
    "f(x) = (x - 3)^2 # an example! root = 3\n",
    "∇f(x) = 2 * (x - 3) # the derivative\n",
    "\n",
    "M = MyGradientDescent(0.1)\n",
    "x = 0.0  # Initial guess, we'll make this random soon\n",
    "\n",
    "init!(M, f, ∇f(x), x)\n",
    "\n",
    "iteration_limit = 3000\n",
    "for i in 1:iteration_limit\n",
    "    x = step!(M, f, ∇f, x)\n",
    "end\n",
    "\n",
    "@printf(\"Found solution: x ≈ %2.7f\\n\", x)\n",
    "f_calls, g_calls, iterations, converged = M.func_calls, M.grad_calls, M.iterations, M.converged\n",
    "@printf(\"Metrics: %d function calls, %d grad calls, %2.7f error in %d iterations \\n\", f_calls, g_calls, converged, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c01709",
   "metadata": {},
   "source": [
    "### Problem 1b\n",
    "\n",
    "Now we'll create a custom version of the conjugate gradient implementation presented in the book. Just like before, we'll start with the version the book lays out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1917c5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W algorithm 4.2 Code! Word for word, no modifications \n",
    "function backtracking_line_search(f, ∇f, x, d, α; p=0.5, β=1e-4)\n",
    "    y, g = f(x), ∇f(x)\n",
    "    \n",
    "    while f(x + α*d) > y + β*α*(g⋅d)\n",
    "        α *= p\n",
    "    end\n",
    "    \n",
    "    return α\n",
    "end\n",
    "\n",
    "\n",
    "# K&W algorithm 5.2 Code! \n",
    "# Modified to use backtracking / Approximate line search instead\n",
    "# of regular line search based on univariate minimization\n",
    "# + Some comments for me to understand it later\n",
    "mutable struct ConjugateGradientDescent <: DescentMethod\n",
    "    d # previous gradient direction\n",
    "    g # previous gradient vector\n",
    "end\n",
    "\n",
    "function init!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    M.g = ∇f(x)\n",
    "    M.d = -M.g\n",
    "    return M\n",
    "end\n",
    "\n",
    "function step!(M::ConjugateGradientDescent, f, ∇f, x)\n",
    "    d, g = M.d, M.g\n",
    "    g′ = ∇f(x)\n",
    "    β = max(0, dot(g′, g′-g)/dot(g, g))\n",
    "    d′ = -g′ + β*d\n",
    "    x′ = x + backtracking_line_search(f, ∇f, x, d′, 10) * d′  # modified to use backtracking line search !!\n",
    "    M.d, M.g = d′, g′\n",
    "    return x′\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6c963",
   "metadata": {},
   "source": [
    "Just like before, we'll run this example to show it could be utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5089ce5",
   "metadata": {},
   "source": [
    "#### Explaination\n",
    "\n",
    "Just like before, we're reusing the interface definition from earlier $DescentMethod$. The struct was updated to be mutable, since it'll hold the previous calculations. The $init$ function was also change to calculate an initial value for the struct. This assigns the initial direction and gradient vector itself.\n",
    "\n",
    "A small modification was made to the step function to handle the backtracking/approximate line search instead of the originial line searched based on univariate optimization. This meant calculating the change in step size, and applying it manually instead of reassigning step size directly. (Since backtracking line search returns how we should modify the step size, rather than the step size directly).\n",
    "\n",
    "Everything was unchanged from the book's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d950cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = 3.7500000, f(x) = 0.5625000\n",
      "Iteration 2: x = 3.7500000, f(x) = 0.5625000\n",
      "Iteration 3: x = 2.8125000, f(x) = 0.0351562\n",
      "Iteration 4: x = 2.8125000, f(x) = 0.0351562\n",
      "Iteration 5: x = 3.0468750, f(x) = 0.0021973\n",
      "Iteration 6: x = 3.0468750, f(x) = 0.0021973\n",
      "Iteration 7: x = 2.9882812, f(x) = 0.0001373\n",
      "Iteration 8: x = 2.9882812, f(x) = 0.0001373\n",
      "Iteration 9: x = 3.0029297, f(x) = 0.0000086\n",
      "Iteration 10: x = 3.0029297, f(x) = 0.0000086\n",
      "Iteration 11: x = 2.9992676, f(x) = 0.0000005\n",
      "Iteration 12: x = 2.9992676, f(x) = 0.0000005\n",
      "Iteration 13: x = 3.0001831, f(x) = 0.0000000\n",
      "Iteration 14: x = 3.0001831, f(x) = 0.0000000\n",
      "Iteration 15: x = 2.9999542, f(x) = 0.0000000\n",
      "Iteration 16: x = 2.9999542, f(x) = 0.0000000\n",
      "Iteration 17: x = 3.0000114, f(x) = 0.0000000\n",
      "Iteration 18: x = 3.0000114, f(x) = 0.0000000\n",
      "Iteration 19: x = 2.9999971, f(x) = 0.0000000\n",
      "Iteration 20: x = 2.9999971, f(x) = 0.0000000\n",
      "Found solution: x ≈ 2.9999971\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the K&W algorithm 5.2 Code w/ backtracking line search! \n",
    "f(x) = (x - 3)^2 # an example! root = 3\n",
    "∇f(x) = 2 * (x - 3) # the derivative\n",
    "\n",
    "M = ConjugateGradientDescent(0.1, 0.1)\n",
    "x = 0.0  # Initial guess, we'll make this random soon\n",
    "\n",
    "init!(M, f, ∇f, x)\n",
    "\n",
    "for i in 1:20\n",
    "    x = step!(M, f, ∇f, x)\n",
    "    @printf(\"Iteration %d: x = %2.7f, f(x) = %2.7f\\n\", i, x, f(x))\n",
    "end\n",
    "\n",
    "@printf(\"Found solution: x ≈ %2.7f\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23043b31",
   "metadata": {},
   "source": [
    "#### Instrumentation!\n",
    "\n",
    "Again, just like before, we'll adjust the structure to keep track of function calls, gradient calls, and iteration count! Since these will ultimately be used to evaluate the gradient methods against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4814c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 4 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W algorithm 5.2 Code! \n",
    "# Modified to use backtracking / Approximate line search instead\n",
    "# of regular line search based on univariate minimization\n",
    "# Also modified to keep track of function calls via structure memebers\n",
    "mutable struct MyConjugateGradientDescent <: DescentMethod\n",
    "    d::Vector{Float64} # previous gradient direction\n",
    "    g::Vector{Float64} # previous gradient vector\n",
    "    \n",
    "    # instrumention stuff\n",
    "    func_calls::Int\n",
    "    grad_calls::Int\n",
    "    iterations::Int\n",
    "    converged::Float64\n",
    "\n",
    "    # Constructor with optional instrumentation fields defaulting to zero/false\n",
    "    function MyConjugateGradientDescent()\n",
    "        new([], [], 0, 0, 0, 0.0)\n",
    "    end\n",
    "end\n",
    "\n",
    "# K&W algorithm 4.2 Code!\n",
    "# Modified to keep track of function calls, and gradient calls\n",
    "# using a common struct that's passed in!\n",
    "function my_backtracking_line_search(M::MyConjugateGradientDescent, f, ∇f, x, d, α; p=0.5, β=1e-4)\n",
    "    y, g = f(x), ∇f(x)\n",
    "    M.func_calls += 1\n",
    "    M.grad_calls += 1\n",
    "    \n",
    "    while f(x + α*d) > y + β*α*dot(g,d)\n",
    "        M.func_calls += 1\n",
    "        α *= p\n",
    "    end\n",
    "    \n",
    "    return α\n",
    "end\n",
    "\n",
    "function init!(M::MyConjugateGradientDescent, f, ∇f, x)\n",
    "    #zero out instrumentation if reusing M\n",
    "    M.func_calls = 0\n",
    "    M.grad_calls = 0\n",
    "    M.iterations = 0\n",
    "    M.converged = 0.0\n",
    "    \n",
    "    # regular Init for conjugate gradient descent\n",
    "    M.g = ∇f(x)\n",
    "    M.grad_calls += 1\n",
    "    M.d = -M.g\n",
    "    return M\n",
    "end\n",
    "\n",
    "function step!(M::MyConjugateGradientDescent, f, ∇f, x; tol=1e-6, max_step_size=10)\n",
    "    \n",
    "    # check for early exit\n",
    "    if( M.converged > 0 )\n",
    "        return x\n",
    "    end\n",
    "    \n",
    "    # let's do an iteration!\n",
    "    M.iterations += 1\n",
    "    \n",
    "    d, g = M.d, M.g\n",
    "    g′ = ∇f(x)\n",
    "    M.grad_calls += 1\n",
    "    \n",
    "    β = max(0, dot(g′, g′-g)/dot(g, g))\n",
    "    d′ = -g′ + β*d\n",
    "    x′ = x + my_backtracking_line_search(M, f, ∇f, x, d′, max_step_size) * d′  # modified to use backtracking line search !!\n",
    "    M.d, M.g = d′, g′\n",
    "    \n",
    "    # tolerance check, maybe we're done! If the magnitude / norm of the gradient is small\n",
    "    # we're probably in the minima (or saddle point)\n",
    "    if( norm(M.g) < tol )\n",
    "        M.converged = norm(M.g)\n",
    "    end\n",
    "    \n",
    "    return x′\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca47b8",
   "metadata": {},
   "source": [
    "### Problem 1c\n",
    "\n",
    "Experiment time! Definte the rosenbrock function and it's gradient - and we'll use the booth function like the last homework. Note, these are normally 2D problems, but to extend these problems to more dimensions, we effectively treat each variable pair wise. So, $x_1$, $x_2$ are paired, $x2$, $x3$ are paired, etc until $x_n$ is reached. In effect, sequential variables are coupled together. And the results are summed together to evaluate the function. \n",
    "\n",
    "Note: In order to use ForwardDiff package, type information must be removed from parameters. So preallocating array isn't possible any the type is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558a6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock_n_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-dimensional Rosenbrock! (as specified by assignment)\n",
    "function rosenbrock_n(x; a = 1.0, b = 5.0, n = 10)\n",
    "    # Compute the sum\n",
    "    result = 0.0\n",
    "    # we end at n-1, because we need the last entry to do n-1th entry\n",
    "    for i in 1:(n - 1)\n",
    "        result += (a - x[i])^2 + b * (x[i + 1] - x[i]^2)^2\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "function rosenbrock_n_gradient(x; a = 1.0, b = 5.0, n = 10)\n",
    "    grad = Float64[]\n",
    "\n",
    "    # Special case for the first element\n",
    "    push!( grad, -2 * (a - x[1]) + (-4) * b * x[1] * ( x[2] - x[1]^2) )\n",
    "    \n",
    "    # Compute the gradient for the 2:n-1 elements\n",
    "    for i in 2:(n-1)\n",
    "        push!( grad, ( -2*(a - x[i]) + 2 * b * (x[i+1]-x[i]^2) * -2 * x[i] ) + 2 * b * (x[i] - x[i-1]^2) )\n",
    "    end\n",
    "\n",
    "    # Special case for the last element (n-th element)\n",
    "    push!( grad, 2 * b * (x[n] - x[n-1]^2) )\n",
    "\n",
    "    return grad\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3eea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "booths_n_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function booths_n(x; n = 10)\n",
    "    result = 0.0\n",
    "    # same rationale for ending at n-1, we need the nth element for n-1\n",
    "    for i in 1:(n - 1)\n",
    "        result += ((x[i] + 2*x[i+1] - 7)^2 + (2*x[i] + x[i+1] - 5)^2)\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "function booths_n_gradient(x; n = 10)\n",
    "    grad = Float64[]\n",
    "    \n",
    "    # Special case for the first element\n",
    "    push!( grad, 2*(x[1]+2*x[2]-7) + 4*(2*x[1] + x[2] - 5) )\n",
    "    \n",
    "    # Compute the gradient for the 2:n-1 elements\n",
    "    for i in 2:(n-1)\n",
    "        push!( grad, 2*(x[i]+2*x[i+1]-7) + 4*(2*x[i] + x[i+1] - 5) + 4*(x[i-1] + 2*x[i] - 7) + 2*(2*x[i-1] + x[i] - 5) )\n",
    "    end\n",
    "\n",
    "    # Special case for the last element (n-th element)\n",
    "    push!( grad, 4*(x[n-1] + 2*x[n] - 7) + 2*(2*x[n-1] + x[n] - 5) ) \n",
    "    \n",
    "    return grad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e6d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 error of rosenbrock: 0.000000 \n",
      "L2 error of booth: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# check gradients using auto diff, since that has no error!\n",
    "x = [i/5 for i in 1:10]\n",
    "@printf(\"L2 error of rosenbrock: %f \\n\", sum((rosenbrock_n_gradient(x) - ForwardDiff.gradient(rosenbrock_n, x)).^2))\n",
    "@printf(\"L2 error of booth: %f\\n\", sum((booths_n_gradient(x) - ForwardDiff.gradient(booths_n, x)).^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65188aa",
   "metadata": {},
   "source": [
    "## Experimental Set Up\n",
    "\n",
    "With both functions and their gradients defined, we can begin to set up the experiment. The stopping criteria here is gradient convergence. The idea being that if we've reached a minima, the norm of the gradient vector should be tiny. I opted for this definition of convergence since it's possible that there may be little change in experimental variables but massive changes in slope (which means we're still optimizing!). \n",
    "\n",
    "To do the actual experiment, we'll generate 100 different starting points, varying from -10 to 10. These will all be randomly generated. In order to report error, I need to have an answer to compare against. I'll use my code from the previous assignment, but updating it to have a parameter for the number of dimensions per random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "722c7830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_initial_points (generic function with 5 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_initial_points(left_limit=-10.0, right_limit=10.0, samples=100, dims=10)\n",
    "    # Generate 100 random real-valued starting points as specified\n",
    "    initial_points = [] \n",
    "\n",
    "    for i in 1:samples # defaults to 100 samples\n",
    "        this_sample = []\n",
    "        \n",
    "        for j in 1:dims\n",
    "            # y is uniformly picked in [-10.0, 10.0] by default\n",
    "            # note, rand is already uniform, just need to cap the limits\n",
    "            y = rand(Uniform(left_limit, right_limit))\n",
    "            # choose + or - with equal probability, we'll use bitrand (50/50)\n",
    "            sign = bitrand() == 0 ? -1 : 1\n",
    "            x = sign * exp(y)\n",
    "            push!(this_sample, x)\n",
    "        end\n",
    "        \n",
    "        push!(initial_points, this_sample)\n",
    "    end\n",
    "\n",
    "    return initial_points\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b64d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " Any[56.52559752390986, 0.0019517842076887226, 0.0343311788946354, 439.95881215660245, 9.329145648901044e-5, 4.650768343637638, 0.0033941816256434208, 0.5029550031940542, 0.6544952296363804, 7.517280410403966]\n",
       " Any[8.036359348702595, 0.046619560977479754, 1.180421446592528, 198.3336564463679, 16.426684380533267, 0.005824155868490791, 876.6540339947347, 0.00019011354496189262, 9936.535212502227, 0.001398343023486806]\n",
       " Any[9.0235143722527, 0.5948571442761463, 62.308639338771066, 0.00013834331597270088, 2938.1642755478933, 80.37155958699849, 7.28395481068361e-5, 0.00023069013238338583, 0.013061140290287068, 15.449331104461034]\n",
       " Any[1.9280229238917257, 69.71358449889952, 0.04723373382924827, 12.272310080228438, 4759.96224883146, 3369.5017971046236, 0.009137921397509007, 3841.601994483109, 0.040305692495622596, 0.014275271909965147]\n",
       " Any[0.025204234479678247, 3542.595474580546, 0.4626233323760416, 6.0052427577234036e-5, 2823.5948532855246, 0.08572935429305956, 0.009739432056940429, 0.007736653406538772, 0.036939596348906124, 0.6146307127489641]\n",
       " Any[0.10594407526024731, 3940.1354284842455, 5482.640882810962, 0.0001756170391424807, 5.076079204323113, 5854.393787432701, 2.12339465925322, 84.63388868897451, 849.4175614200315, 9.677355343927011e-5]\n",
       " Any[21664.57479347603, 124.15672369161588, 472.5273037348895, 0.317628475181968, 9.037262257911571, 606.3067009216386, 0.13371655496806636, 0.0014332057658767794, 3818.307320131547, 589.0243519909808]\n",
       " Any[392.4471238963144, 0.0013706688275159282, 7853.684843373571, 0.19893860994131898, 0.00019769892105153696, 0.0005843939669083492, 0.46101535090822465, 8.2469076231856e-5, 0.00012707472865654748, 0.0005023239386618092]\n",
       " Any[8.058837000611414, 3266.9192769557308, 1.1751517143312857, 1.4914826847842777, 0.04170696152216765, 714.0451006864039, 0.017804773768269295, 209.72267785349342, 441.3937179617702, 0.018817448851875366]\n",
       " Any[0.11551355419094916, 0.0725053965485163, 6.312181489409248e-5, 0.00019189573163484234, 0.8719381696742253, 0.00010228870740905941, 0.0021763801203377457, 1.6664921669691086, 18985.374121221972, 0.00010274789461683367]\n",
       " Any[0.31597319718983624, 0.0002950312672405904, 0.0018271775601155819, 267.51014437062463, 208.65188083092474, 2575.691833709852, 0.00010021392288709919, 0.0005791263586267163, 1.1304603671186757, 2831.3963947366733]\n",
       " Any[7.944686492670709, 7109.917135868649, 0.03430594935867437, 0.00029525837586714515, 0.7966540291126559, 79.931054141564, 186.68656954484302, 0.00044925162321169363, 2.7852903368424227, 1.4484740635480864]\n",
       " Any[0.03422454138323921, 0.0006109074941635463, 680.1731197847421, 2.52739368036556, 0.0001540184521910178, 0.05205541343206713, 2079.1982821687284, 1.2493210212643902, 1.4173033755842532, 0.00811495929715633]\n",
       " ⋮\n",
       " Any[0.013765945112272358, 17.657817370851443, 0.08565258677409061, 0.05477124514126286, 0.05836187495308221, 7696.639098218069, 17517.532597776546, 0.00012167072647464145, 0.08614504336890563, 0.003922656455286799]\n",
       " Any[0.6952687194771586, 32.20957106031436, 0.40111992380474343, 0.4837702351665244, 261.95210835882887, 2.9141522473104176, 0.3939382493935111, 0.0017958626345316567, 0.0023990513180643905, 0.65522533085463]\n",
       " Any[0.0301862229948371, 2573.1646285846596, 0.942892896786192, 39.196227593290395, 1.334235647222553, 0.0003455907271960224, 2183.670599864225, 87.6602094815643, 0.05870081461555172, 0.00012841805647752265]\n",
       " Any[6.0049600367611976e-5, 0.20887992313322631, 0.000147923762925379, 6198.856498181542, 0.00016342136472823456, 178.92590821866898, 8.94645951398003e-5, 15969.971669146013, 1.4549792473201282, 0.00709869250614748]\n",
       " Any[0.1688010682981114, 0.44340054790215927, 12897.79426567388, 0.006269325007582176, 0.025851305073994652, 2365.6543616885488, 65.35349682337284, 0.001786309965864828, 0.0016508718475116888, 3351.157339610774]\n",
       " Any[6998.3522689545725, 544.9370070366557, 0.0005553691335545747, 0.0020939763579684045, 841.6383677679302, 0.6010755275848438, 22.86110818429628, 0.00018046450041960018, 0.0036263708018198946, 0.0011052295282245948]\n",
       " Any[10.966120829238138, 12738.317229504075, 0.00015635212665630923, 31.897014435925254, 0.007869203356665843, 0.00012634773459570465, 0.00043382370720694867, 0.002400636785523877, 1.569981499379057, 0.41597678127490023]\n",
       " Any[113.74510305732532, 0.1569513990429607, 0.8083109249779732, 6.73786855332635e-5, 12.661428447054345, 0.000599780335954372, 0.1695209124391354, 2947.0249106297024, 0.0010254240969272343, 490.4565961308441]\n",
       " Any[0.02462034702612805, 0.2872335123469971, 7652.705965478392, 1140.9442145629034, 10161.43001441691, 1.486896234646811, 0.9702700323276914, 456.4217385290914, 3483.834815982465, 8999.594323966205]\n",
       " Any[0.0001629565114454101, 15071.963184591054, 13.542168023855556, 9.189052990847766e-5, 1.2721892660642486, 0.39491306559147116, 1151.6968638086687, 3.0716830781605697, 0.033966752631606645, 911.964143662509]\n",
       " Any[0.0002658033545277094, 0.002205263710029156, 1.5488984996460735, 9.695114977442515e-5, 0.00025952061024217353, 6.50294276286769e-5, 6730.318594672222, 2057.5027383780134, 19.69359577513823, 0.2599544094203514]\n",
       " Any[1.2641239433948581, 1.1060310448903727, 30.532027767157484, 0.0002507550675207203, 5.72007342100888e-5, 0.12431940528014426, 0.009315774014422427, 7107.984786405302, 10.867873911170276, 2313.898862252177]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_initial_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1236537b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lists to store results for entire run\n",
    "results_grad_rosenbrock = []\n",
    "results_conj_rosenbrock = []\n",
    "results_grad_booths = []\n",
    "results_conj_booths = []\n",
    "\n",
    "# Generate initial points for each function we'll be testing\n",
    "initial_points_rosenbrock = generate_initial_points()\n",
    "initial_points_booths = generate_initial_points()\n",
    "\n",
    "# Create the Models we'll use (our algos)\n",
    "gradient = MyGradientDescent(0.5)\n",
    "conjugate = MyConjugateGradientDescent()\n",
    "\n",
    "# Get our solutions\n",
    "rosenbrock_opt_res = optimize(rosenbrock_n, convert(Vector{Float32}, initial_points_rosenbrock[1]), LBFGS(); autodiff = :forward)\n",
    "rosenbrock_opt_min_x = Optim.minimizer(rosenbrock_opt_res)\n",
    "\n",
    "# Get our solutions\n",
    "booths_opt_res = optimize(booths_n, convert(Vector{Float32}, initial_points_booths[1]), LBFGS(); autodiff = :forward)\n",
    "booths_opt_min_x = Optim.minimizer(booths_opt_res)\n",
    "\n",
    "# Run the optimizer for each function! Using each model\n",
    "for (M, result_containers) in zip([gradient, conjugate], [(results_grad_rosenbrock,results_grad_booths) , (results_conj_rosenbrock, results_conj_booths)])\n",
    "    \n",
    "    # Lists to store results\n",
    "    results_rosenbrock = []\n",
    "    results_booths = []\n",
    "\n",
    "    for (targets, analytical_solution, points, results) in zip(\n",
    "            [(rosenbrock_n, rosenbrock_n_gradient), (booths_n, booths_n_gradient)], \n",
    "            [rosenbrock_opt_min_x, booths_opt_min_x], \n",
    "            [initial_points_rosenbrock, initial_points_booths], \n",
    "            [results_rosenbrock, results_booths])    \n",
    "\n",
    "        (func, func_grad) = targets\n",
    "\n",
    "        for x in points\n",
    "            start_time = time()\n",
    "            init!(M, func, func_grad, x)\n",
    "\n",
    "            iteration_limit = 1000\n",
    "            for i in 1:iteration_limit\n",
    "                x = step!(M, func, func_grad, x)\n",
    "                \n",
    "                # Early Exit if we have disappearing gradient or exploding gradient\n",
    "                if any(isnan, x) || any(isinf, x)\n",
    "                    break  # stop processing samples!\n",
    "                end\n",
    "            end\n",
    "\n",
    "            elapsed_time = time() - start_time\n",
    "            f_calls, g_calls, iterations, converged = M.func_calls, M.grad_calls, M.iterations, M.converged\n",
    "            \n",
    "            absolute_error = norm(x - analytical_solution, Inf)  # Infinity norm for max absolute error\n",
    "            \n",
    "            push!(results, (absolute_error, converged, f_calls + g_calls, elapsed_time))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    (rosen, booth) = result_containers\n",
    "    \n",
    "    append!(rosen, results_rosenbrock)\n",
    "    append!(booth, results_booths)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe748bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Any}:\n",
       " (Inf, 0.0, 4, 0.17285895347595215)\n",
       " (NaN, 0.0, 5, 7.891654968261719e-5)\n",
       " (Inf, 0.0, 5, 1.5974044799804688e-5)\n",
       " (NaN, 0.0, 5, 1.1920928955078125e-5)\n",
       " (NaN, 0.0, 5, 1.0967254638671875e-5)\n",
       " (NaN, 0.0, 5, 1.0967254638671875e-5)\n",
       " (Inf, 0.0, 4, 9.059906005859375e-6)\n",
       " (Inf, 0.0, 4, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 8.821487426757812e-6)\n",
       " (NaN, 0.0, 5, 1.1920928955078125e-5)\n",
       " (NaN, 0.0, 5, 1.0013580322265625e-5)\n",
       " (NaN, 0.0, 5, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 5, 1.0013580322265625e-5)\n",
       " ⋮\n",
       " (Inf, 0.0, 4, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 9.059906005859375e-6)\n",
       " (Inf, 0.0, 4, 1.0967254638671875e-5)\n",
       " (Inf, 0.0, 4, 9.059906005859375e-6)\n",
       " (NaN, 0.0, 5, 1.2159347534179688e-5)\n",
       " (NaN, 0.0, 5, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 9.059906005859375e-6)\n",
       " (NaN, 0.0, 5, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 1.0013580322265625e-5)\n",
       " (Inf, 0.0, 4, 9.059906005859375e-6)\n",
       " (Inf, 0.0, 4, 1.0013580322265625e-5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_grad_rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df76a0a",
   "metadata": {},
   "source": [
    "Process the results into the table required by the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdb1066",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────┬─────────────────────────────┬──────────────────────────┬──────────────────────────┬────────────────────────┐\n",
      "│\u001b[1m  Function \u001b[0m│\u001b[1m Absolute Error (mean ± std) \u001b[0m│\u001b[1m Convergence (mean ± std) \u001b[0m│\u001b[1m Evaluations (mean ± std) \u001b[0m│\u001b[1m Time (ms) (mean ± std) \u001b[0m│\n",
      "├───────────┼─────────────────────────────┼──────────────────────────┼──────────────────────────┼────────────────────────┤\n",
      "│ Rosen(GD) │                   Inf ± NaN │      0.00e+00 ± 0.00e+00 │              4.43 ± 0.52 │           1.74 ± 17.28 │\n",
      "│ Rosen(CD) │         1.02e+03 ± 2.99e+03 │      6.40e-07 ± 3.98e-07 │      12603.31 ± 11099.33 │           5.81 ± 25.64 │\n",
      "│ Booth(GD) │                   Inf ± NaN │      0.00e+00 ± 0.00e+00 │            250.65 ± 0.78 │            0.85 ± 5.64 │\n",
      "│ Booth(CD) │         2.94e-07 ± 1.14e-08 │      6.38e-07 ± 1.96e-07 │          999.31 ± 154.22 │           2.32 ± 14.21 │\n",
      "└───────────┴─────────────────────────────┴──────────────────────────┴──────────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "function summarize_results(results; nan_replacement=nothing)\n",
    "    # Initialize arrays to collect numerical data\n",
    "    absolute_errors = Float64[]\n",
    "    convergences = Float64[]\n",
    "    times = Float64[]\n",
    "    evaluations = Int[]\n",
    "\n",
    "    for r in results\n",
    "        # Extract data from r[1], r[2], and r[4]\n",
    "        # They may be numbers or iterables (tuples/arrays)\n",
    "\n",
    "        # Process r[1]: absolute_errors\n",
    "        data_r1 = r[1]\n",
    "        values_r1 = isa(data_r1, Number) ? [data_r1] : collect(data_r1)\n",
    "\n",
    "        # Process r[2]: convergences\n",
    "        data_r2 = r[2]\n",
    "        values_r2 = isa(data_r2, Number) ? [data_r2] : collect(data_r2)\n",
    "\n",
    "        # Process r[4]: times\n",
    "        data_r4 = r[4]\n",
    "        values_r4 = isa(data_r4, Number) ? [data_r4] : collect(data_r4)\n",
    "\n",
    "        # Process r[3]: evaluations (assumed to be valid)\n",
    "        evaluations_r = r[3]\n",
    "\n",
    "        # Handle NaNs\n",
    "        if isnothing(nan_replacement)\n",
    "            # Skip NaN values\n",
    "            values_r1 = [v for v in values_r1 if !isnan(v)]\n",
    "            values_r2 = [v for v in values_r2 if !isnan(v)]\n",
    "            values_r4 = [v for v in values_r4 if !isnan(v)]\n",
    "        else\n",
    "            # Replace NaNs with the specified replacement value\n",
    "            values_r1 = [isnan(v) ? nan_replacement : v for v in values_r1]\n",
    "            values_r2 = [isnan(v) ? nan_replacement : v for v in values_r2]\n",
    "            values_r4 = [isnan(v) ? nan_replacement : v for v in values_r4]\n",
    "        end\n",
    "\n",
    "        # Append numerical values to arrays\n",
    "        append!(absolute_errors, values_r1)\n",
    "        append!(convergences, values_r2)\n",
    "        append!(times, values_r4)\n",
    "        push!(evaluations, evaluations_r)\n",
    "    end\n",
    "\n",
    "    # Compute statistics, handling empty lists gracefully\n",
    "    mean_absolute_error = isempty(absolute_errors) ? NaN : mean(absolute_errors)\n",
    "    std_absolute_error = isempty(absolute_errors) ? NaN : std(absolute_errors)\n",
    "    mean_convergence = isempty(convergences) ? NaN : mean(convergences)\n",
    "    std_convergence = isempty(convergences) ? NaN : std(convergences)\n",
    "    mean_evaluations = mean(evaluations)\n",
    "    std_evaluations = std(evaluations)\n",
    "    mean_time = isempty(times) ? NaN : mean(times) * 1e3  # Convert to milliseconds\n",
    "    std_time = isempty(times) ? NaN : std(times) * 1e3    # Convert to milliseconds\n",
    "\n",
    "    return (mean_absolute_error, std_absolute_error,\n",
    "            mean_convergence, std_convergence,\n",
    "            mean_evaluations, std_evaluations,\n",
    "            mean_time, std_time)\n",
    "end\n",
    "\n",
    "\n",
    "# Summarize results for each function\n",
    "summary_rosen_grad = summarize_results(results_grad_rosenbrock)\n",
    "summary_rosen_conj = summarize_results(results_conj_rosenbrock)\n",
    "summary_booth_grad = summarize_results(results_grad_booths)\n",
    "summary_booth_conj = summarize_results(results_conj_booths)\n",
    "\n",
    "# Print results in a 4x4 table\n",
    "# Create a Table to display results as a table\n",
    "using PrettyTables\n",
    "\n",
    "headers = [\"Function\", \"Absolute Error (mean ± std)\", \"Convergence (mean ± std)\", \"Evaluations (mean ± std)\", \"Time (ms) (mean ± std)\"]\n",
    "fdata = hcat(\"Rosen(GD)\", @sprintf(\"%.2e ± %.2e\", summary_rosen_grad[1], summary_rosen_grad[2]), @sprintf(\"%.2e ± %.2e\", summary_rosen_grad[3], summary_rosen_grad[4]), @sprintf(\"%.2f ± %.2f\", summary_rosen_grad[5], summary_rosen_grad[6]), @sprintf(\"%.2f ± %.2f\", summary_rosen_grad[7], summary_rosen_grad[8]))\n",
    "gdata = hcat(\"Rosen(CD)\", @sprintf(\"%.2e ± %.2e\", summary_rosen_conj[1], summary_rosen_conj[2]), @sprintf(\"%.2e ± %.2e\", summary_rosen_conj[3], summary_rosen_conj[4]), @sprintf(\"%.2f ± %.2f\", summary_rosen_conj[5], summary_rosen_conj[6]), @sprintf(\"%.2f ± %.2f\", summary_rosen_conj[7], summary_rosen_conj[8]))\n",
    "hdata = hcat(\"Booth(GD)\", @sprintf(\"%.2e ± %.2e\", summary_booth_grad[1], summary_booth_grad[2]), @sprintf(\"%.2e ± %.2e\", summary_booth_grad[3], summary_booth_grad[4]), @sprintf(\"%.2f ± %.2f\", summary_booth_grad[5], summary_booth_grad[6]), @sprintf(\"%.2f ± %.2f\", summary_booth_grad[7], summary_booth_grad[8]))\n",
    "jdata = hcat(\"Booth(CD)\", @sprintf(\"%.2e ± %.2e\", summary_booth_conj[1], summary_booth_conj[2]), @sprintf(\"%.2e ± %.2e\", summary_booth_conj[3], summary_booth_conj[4]), @sprintf(\"%.2f ± %.2f\", summary_booth_conj[5], summary_booth_conj[6]), @sprintf(\"%.2f ± %.2f\", summary_booth_conj[7], summary_booth_conj[8]))\n",
    "\n",
    "\n",
    "pretty_table(vcat(fdata, gdata, hdata, jdata), header=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1fe9a",
   "metadata": {},
   "source": [
    "We didn't achieve convergence at all with Gradient Descent (GD). While with Conjugate Descent, we got convergence, at the cost of much much more computation. Note, we experimenced disappearing gradients with the regular GD algorithm, resuling in NaNs or Infs when doing the derviate and applying the learning rate update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c20e1",
   "metadata": {},
   "source": [
    "## Problem 2, An advanced optimizer! Enhancing GD\n",
    "\n",
    "Since I'm a grad student, I'll have to do problem 2! For this problem, I'll go ahead and implement one of the various algorithms in Chapter 5 of the book. Basically, all the remaining methods add momentum. I'll go with Nesterov Momentum since it's a bit out of vogue. It's Algorithm 5.4 from K&W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd09cee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 5 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taken from Algorithm 5.4 in K&W, no modifications made\n",
    "\n",
    "mutable struct NesterovMomentum <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    v # momentum\n",
    "end\n",
    "\n",
    "function init!(M::NesterovMomentum, f, ∇f, x)\n",
    "    M.v = zeros(length(x))\n",
    "    return M\n",
    "end\n",
    "\n",
    "function step!(M::NesterovMomentum, f, ∇f, x)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    v[:] = β*v - α*∇f(x + β*v)\n",
    "    return x + v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274311b",
   "metadata": {},
   "source": [
    "Like the other models before, we'll go ahead and instrument this up to keep track of various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a97e59d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_stats (generic function with 2 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taken from Algorithm 5.4 in K&W\n",
    "# Added instrumentation! For function evals and for iteration count\n",
    "\n",
    "mutable struct MyNesterovMomentum <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    v # momentum\n",
    "    \n",
    "    # instrumention stuff\n",
    "    func_calls::Int\n",
    "    grad_calls::Int\n",
    "    iterations::Int\n",
    "    converged::Float64\n",
    "\n",
    "    # Constructor with optional instrumentation fields defaulting to zero/false\n",
    "    function MyNesterovMomentum(α, β)\n",
    "        new(α, β, [], 0, 0, 0, 0.0)\n",
    "    end\n",
    "end\n",
    "\n",
    "function init!(M::MyNesterovMomentum, f, ∇f, x)\n",
    "    M.v = zeros(length(x))\n",
    "    \n",
    "    # instrumention stuff, if we reuse M\n",
    "    func_calls = 0\n",
    "    grad_calls = 0\n",
    "    iterations = 0\n",
    "    converged = 0.0\n",
    "    \n",
    "    return M\n",
    "end\n",
    "\n",
    "function step!(M::MyNesterovMomentum, f, ∇f, x)\n",
    "    if( M.converged > 0.0 )\n",
    "        return x\n",
    "    end\n",
    "    \n",
    "    M.iterations += 1\n",
    "    \n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    v[:] = β*v - α*∇f(x + β*v)\n",
    "    M.grad_calls += 1\n",
    "    \n",
    "    return x + v\n",
    "end\n",
    "\n",
    "# one liner stats call\n",
    "get_stats(M::MyNesterovMomentum) = M.func_calls, M.grad_calls, M.iterations, M.converged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4854c42",
   "metadata": {},
   "source": [
    "Let's run the same experiment! But this time with our brand new Optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "717e411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results for entire run\n",
    "results_nest_rosenbrock = []\n",
    "results_nest_booths = []\n",
    "\n",
    "# Generate initial points for each function we'll be testing\n",
    "initial_points_rosenbrock = generate_initial_points()\n",
    "initial_points_booths = generate_initial_points()\n",
    "\n",
    "# Create the Models we'll use (our algos)\n",
    "nest = MyNesterovMomentum(0.0001, 0.5)\n",
    "\n",
    "# Get our solutions\n",
    "rosenbrock_opt_res = optimize(rosenbrock_n, convert(Vector{Float32}, initial_points_rosenbrock[1]), LBFGS(); autodiff = :forward)\n",
    "rosenbrock_opt_min_x = Optim.minimizer(rosenbrock_opt_res)\n",
    "\n",
    "# Get our solutions\n",
    "booths_opt_res = optimize(booths_n, convert(Vector{Float32}, initial_points_booths[1]), LBFGS(); autodiff = :forward)\n",
    "booths_opt_min_x = Optim.minimizer(booths_opt_res)\n",
    "\n",
    "# Lists to store results\n",
    "results_rosenbrock = []\n",
    "results_booths = []\n",
    "\n",
    "for (targets, analytical_solution, points, results) in zip(\n",
    "        [(rosenbrock_n, rosenbrock_n_gradient), (booths_n, booths_n_gradient)], \n",
    "        [rosenbrock_opt_min_x, booths_opt_min_x], \n",
    "        [initial_points_rosenbrock, initial_points_booths], \n",
    "        [results_rosenbrock, results_booths])    \n",
    "\n",
    "    (func, func_grad) = targets\n",
    "\n",
    "    for x in points\n",
    "        start_time = time()\n",
    "        init!(nest, func, func_grad, x)\n",
    "\n",
    "        iteration_limit = 1000\n",
    "        for i in 1:iteration_limit\n",
    "            x = step!(nest, func, func_grad, x)\n",
    "\n",
    "            # did we converge?\n",
    "            if( norm(func_grad(x)) < 1e-6 )\n",
    "                nest.converged = norm(func_grad(x))\n",
    "            end\n",
    "            \n",
    "            # Early Exit if we have disappearing gradient or exploding gradient\n",
    "            if any(isnan, x) || any(isinf, x)\n",
    "                break  # stop processing samples!\n",
    "            end\n",
    "            \n",
    "        end\n",
    "\n",
    "        elapsed_time = time() - start_time\n",
    "        f_calls, g_calls, iterations, converged = nest.func_calls, nest.grad_calls, nest.iterations, nest.converged\n",
    "\n",
    "        absolute_error = norm(x - analytical_solution, Inf)  # Infinity norm for max absolute error\n",
    "\n",
    "        push!(results, (absolute_error, converged, f_calls + g_calls, elapsed_time))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9479acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────┬─────────────────────────────┬──────────────────────────┬──────────────────────────┬────────────────────────┐\n",
      "│\u001b[1m   Function \u001b[0m│\u001b[1m Absolute Error (mean ± std) \u001b[0m│\u001b[1m Convergence (mean ± std) \u001b[0m│\u001b[1m Evaluations (mean ± std) \u001b[0m│\u001b[1m Time (ms) (mean ± std) \u001b[0m│\n",
      "├────────────┼─────────────────────────────┼──────────────────────────┼──────────────────────────┼────────────────────────┤\n",
      "│  Rosen(ND) │                   Inf ± NaN │      0.00e+00 ± 0.00e+00 │          258.36 ± 148.49 │            0.02 ± 0.01 │\n",
      "│ Booths(ND) │         1.32e+03 ± 1.38e+03 │      0.00e+00 ± 0.00e+00 │      51011.00 ± 29011.49 │            1.77 ± 0.75 │\n",
      "└────────────┴─────────────────────────────┴──────────────────────────┴──────────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Summarize results for each function\n",
    "summary_rosen_grad = summarize_results(results_rosenbrock)\n",
    "summary_rosen_conj = summarize_results(results_booths)\n",
    "\n",
    "# Print results in a 4x4 table\n",
    "# Create a Table to display results as a table\n",
    "using PrettyTables\n",
    "\n",
    "headers = [\"Function\", \"Absolute Error (mean ± std)\", \"Convergence (mean ± std)\", \"Evaluations (mean ± std)\", \"Time (ms) (mean ± std)\"]\n",
    "fdata = hcat(\"Rosen(ND)\", @sprintf(\"%.2e ± %.2e\", summary_rosen_grad[1], summary_rosen_grad[2]), @sprintf(\"%.2e ± %.2e\", summary_rosen_grad[3], summary_rosen_grad[4]), @sprintf(\"%.2f ± %.2f\", summary_rosen_grad[5], summary_rosen_grad[6]), @sprintf(\"%.2f ± %.2f\", summary_rosen_grad[7], summary_rosen_grad[8]))\n",
    "gdata = hcat(\"Booths(ND)\", @sprintf(\"%.2e ± %.2e\", summary_rosen_conj[1], summary_rosen_conj[2]), @sprintf(\"%.2e ± %.2e\", summary_rosen_conj[3], summary_rosen_conj[4]), @sprintf(\"%.2f ± %.2f\", summary_rosen_conj[5], summary_rosen_conj[6]), @sprintf(\"%.2f ± %.2f\", summary_rosen_conj[7], summary_rosen_conj[8]))\n",
    "\n",
    "pretty_table(vcat(fdata, gdata), header=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173fa1a",
   "metadata": {},
   "source": [
    "Very simliar behavior to Gradient Descent for the 10 dimensional Rosenbrock. It even took more iterations than regular gradient descent to get to the same vanishing or exploding gradient problem. Meaning, regular Gradient Descent was able to realize it could never converge quicker. However, this could also mean that Nesterov Momentum was able to actually follow the gradients for a longer duration.\n",
    "\n",
    "What's interesting is Nesterov Momentum could keep iterating long enough to have somewhat real errors with infinite norm! And it did it much faster (about 1 ms faster) than Conjugate descent on average. But it had an order of magnitude more evaluations to get the same result. And the error is quite large when comparing the two. However, using the same convergence metric as earlier (if the norm of the gradient gets small) it's clear that the gradient is still disappearing or exploding!\n",
    "\n",
    "To really get anything useful out of Gradient Descent or Nesterov Momentum Descent, I think I need to tune the hyper parameters a bit more. For example, messing with the learning rate of Gradient Descent (and the line search), I was able to get to some minima. Another possible route is constraining the design points - getting them closer to where the actual minima is could really help these methods. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
